# Autoware ML Deployment Framework

A unified, task-agnostic deployment framework for exporting, verifying, and evaluating machine learning models across different backends (ONNX, TensorRT).

## Architecture Overview

```
Deployment Framework
â”œâ”€â”€ Core Abstractions
â”‚   â”œâ”€â”€ BaseDataLoader       # Task-specific data loading
â”‚   â”œâ”€â”€ BaseEvaluator        # Task-specific evaluation
â”‚   â”œâ”€â”€ BaseBackend          # Unified inference interface
â”‚   â””â”€â”€ BaseDeploymentConfig # Configuration management
â”‚
â”œâ”€â”€ Backends
â”‚   â”œâ”€â”€ PyTorchBackend       # PyTorch inference
â”‚   â”œâ”€â”€ ONNXBackend          # ONNX Runtime inference
â”‚   â””â”€â”€ TensorRTBackend      # TensorRT inference
â”‚
â”œâ”€â”€ Exporters
â”‚   â”œâ”€â”€ ONNXExporter         # PyTorch â†’ ONNX
â”‚   â””â”€â”€ TensorRTExporter     # ONNX â†’ TensorRT
â”‚
â””â”€â”€ Project Implementations
    â”œâ”€â”€ CalibrationStatusClassification/deploy/
    â”œâ”€â”€ YOLOX/deploy/  
    â””â”€â”€ CenterPoint/deploy/  
```

---

## ðŸš€ Quick Start

### For Implemented Projects

#### CalibrationStatusClassification

```bash
python projects/CalibrationStatusClassification/deploy/main.py \
    projects/CalibrationStatusClassification/deploy/deploy_config.py \
    projects/CalibrationStatusClassification/configs/t4dataset/resnet18_5ch_1xb16-50e_j6gen2.py \
    checkpoint.pth \
    --work-dir work_dirs/deployment
```

See `projects/CalibrationStatusClassification/deploy/README.md` for details.

---

## ðŸ“š Documentation

- **Design Document**: `/docs/design/deploy_pipeline_design.md`
- **Architecture**: See above
- **Per-Project Guides**: `projects/{PROJECT}/deploy/README.md`

---

## ðŸ”§ Development Guidelines

### Adding a New Project

1. **Create deploy directory**: `projects/{PROJECT}/deploy/`

2. **Implement DataLoader**:
   ```python
   from autoware_ml.deployment.core import BaseDataLoader

   class YourDataLoader(BaseDataLoader):
       def load_sample(self, index: int) -> Dict[str, Any]:
           # Load raw data
           pass

       def preprocess(self, sample: Dict[str, Any]) -> torch.Tensor:
           # Preprocess for model input
           pass

       def get_num_samples(self) -> int:
           return len(self.data)
   ```

3. **Implement Evaluator**:
   ```python
   from autoware_ml.deployment.core import BaseEvaluator

   class YourEvaluator(BaseEvaluator):
       def evaluate(self, model_path, data_loader, ...):
           # Run inference and compute metrics
           pass

       def print_results(self, results):
           # Pretty print results
           pass
   ```

4. **Create deployment config** (`deploy_config.py`)

5. **Create main script** (`main.py`)

6. **Test and document**
