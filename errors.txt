11/12 11:16:52 - mmengine - WARNING - DeprecationWarning: get_onnx_config will be deprecated in the future. 
11/12 11:16:53 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.11.13 | packaged by conda-forge | (main, Jun  4 2025, 14:48:23) [GCC 13.3.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 0
    GPU 0: NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.9, V12.9.86
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.8.0+cu129
    PyTorch compiling details: PyTorch built with:
  - GCC 13.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.9
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120;-gencode;arch=compute_120,code=compute_120
  - CuDNN 91.0.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=a1cb3cc05d46d198467bebbb6e8fba50a325d4e7, CUDA_VERSION=12.9, CUDNN_VERSION=9.10.2, CXX_COMPILER=/opt/rh/gcc-toolset-13/root/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -DC10_NODEPRECATED -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-dangling-reference -Wno-error=dangling-reference -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, 

    TorchVision: 0.23.0+cu129
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl', 'timeout': 21600}
    seed: 0
    diff_rank_seed: False
    deterministic: False
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

11/12 11:16:55 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=16, enable=False)
backend_args = None
camera_order = [
    'CAM_FRONT',
    'CAM_FRONT_LEFT',
    'CAM_FRONT_RIGHT',
    'CAM_BACK_LEFT',
    'CAM_BACK_RIGHT',
]
camera_panels = [
    'data/CAM_FRONT_LEFT',
    'data/CAM_FRONT',
    'data/CAM_FRONT_RIGHT',
    'data/CAM_BACK_LEFT',
    'data/CAM_BACK',
    'data/CAM_BACK_RIGHT',
]
camera_types = {
    'CAM_BACK_LEFT', 'CAM_FRONT_RIGHT', 'CAM_FRONT_LEFT', 'CAM_BACK_RIGHT',
    'CAM_FRONT', 'CAM_BACK'
}
class_colors = dict(
    barrier=(
        0,
        0,
        0,
    ),
    bicycle=(
        255,
        0,
        30,
    ),
    bus=(
        111,
        255,
        111,
    ),
    car=(
        30,
        144,
        255,
    ),
    construction_vehicle=(
        255,
        255,
        0,
    ),
    motorcycle=(
        100,
        0,
        30,
    ),
    pedestrian=(
        255,
        200,
        200,
    ),
    traffic_cone=(
        120,
        120,
        120,
    ),
    trailer=(
        0,
        255,
        255,
    ),
    truck=(
        140,
        0,
        255,
    ))
class_names = [
    'car',
    'truck',
    'bus',
    'bicycle',
    'pedestrian',
]
custom_hooks = [
    dict(disable_after_epoch=15, type='DisableObjectSampleHook'),
]
custom_imports = dict(
    allow_failed_imports=False,
    imports=[
        'projects.BEVFusion.bevfusion',
        'projects.CenterPoint.models',
        'autoware_ml.detection3d.datasets.t4dataset',
        'autoware_ml.detection3d.evaluation.t4metric.t4metric',
        'autoware_ml.detection3d.evaluation.t4metric.t4metric_v2',
        'autoware_ml.detection3d.datasets.transforms',
    ])
data_prefix = dict(
    CAM_BACK='',
    CAM_BACK_LEFT='',
    CAM_BACK_RIGHT='',
    CAM_FRONT='',
    CAM_FRONT_LEFT='',
    CAM_FRONT_RIGHT='',
    pts='',
    sweeps='')
data_root = 'data/t4datasets/'
dataset_test_groups = dict(
    db_j6_gen2_base='t4dataset_j6gen2_base_infos_test.pkl',
    db_j6gen2='t4dataset_j6gen2_infos_test.pkl',
    db_largebus='t4dataset_largebus_infos_test.pkl')
dataset_type = 'T4Dataset'
dataset_version_config_root = 'autoware_ml/configs/t4dataset/'
dataset_version_list = [
    'db_j6gen2_v1',
    'db_j6gen2_v2',
    'db_j6gen2_v3',
    'db_j6gen2_v4',
    'db_j6gen2_v5',
    'db_largebus_v1',
    'db_largebus_v2',
]
default_hooks = dict(
    checkpoint=dict(
        interval=1,
        max_keep_ckpts=10,
        save_best='NuScenes metric/T4Metric/mAP',
        type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='Det3DVisualizationHook'))
default_scope = 'mmdet3d'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl', timeout=21600),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
eval_class_range = dict(
    bicycle=120, bus=120, car=120, pedestrian=120, truck=120)
evaluator_metric_configs = dict(
    center_distance_bev_thresholds=[
        0.5,
        1.0,
        2.0,
        4.0,
    ],
    evaluation_task='detection',
    iou_2d_thresholds=None,
    iou_3d_thresholds=None,
    label_prefix='autoware',
    max_distance=121.0,
    min_distance=-121.0,
    min_point_numbers=0,
    plane_distance_thresholds=[
        2.0,
        4.0,
    ],
    target_labels=[
        'car',
        'truck',
        'bus',
        'bicycle',
        'pedestrian',
    ])
filter_attributes = [
    (
        'vehicle.bicycle',
        'vehicle_state.parked',
    ),
    (
        'vehicle.bicycle',
        'cycle_state.without_rider',
    ),
    (
        'vehicle.bicycle',
        'motorcycle_state.without_rider',
    ),
    (
        'vehicle.motorcycle',
        'vehicle_state.parked',
    ),
    (
        'vehicle.motorcycle',
        'cycle_state.without_rider',
    ),
    (
        'vehicle.motorcycle',
        'motorcycle_state.without_rider',
    ),
    (
        'bicycle',
        'vehicle_state.parked',
    ),
    (
        'bicycle',
        'cycle_state.without_rider',
    ),
    (
        'bicycle',
        'motorcycle_state.without_rider',
    ),
    (
        'motorcycle',
        'vehicle_state.parked',
    ),
    (
        'motorcycle',
        'cycle_state.without_rider',
    ),
    (
        'motorcycle',
        'motorcycle_state.without_rider',
    ),
]
filter_cfg = dict(filter_frames_with_camera_order=[
    'CAM_FRONT',
    'CAM_FRONT_LEFT',
    'CAM_FRONT_RIGHT',
    'CAM_BACK_LEFT',
    'CAM_BACK_RIGHT',
])
grid_size = [
    1440,
    1440,
    41,
]
image_size = [
    480,
    640,
]
info_directory_path = 'info/kokseang_2_3/'
info_test_file_name = 't4dataset_j6gen2_base_infos_test.pkl'
info_train_file_name = 't4dataset_j6gen2_base_infos_train.pkl'
info_val_file_name = 't4dataset_j6gen2_base_infos_val.pkl'
input_modality = dict(use_camera=True, use_lidar=True)
launcher = 'none'
lidar_feature_dims = 4
lidar_sweep_dims = [
    0,
    1,
    2,
    4,
]
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
lr = 0.0001
max_epochs = 100
max_num_points = 10
max_voxels = [
    120000,
    160000,
]
merge_objects = [
    (
        'truck',
        [
            'truck',
            'trailer',
        ],
    ),
]
merge_type = 'extend_longer'
metainfo = dict(classes=[
    'car',
    'truck',
    'bus',
    'bicycle',
    'pedestrian',
])
model = dict(
    bbox_head=dict(
        auxiliary=True,
        bbox_coder=dict(
            code_size=10,
            out_size_factor=8,
            pc_range=[
                -122.4,
                -122.4,
            ],
            post_center_range=[
                -200.0,
                -200.0,
                -10.0,
                200.0,
                200.0,
                10.0,
            ],
            score_threshold=0.0,
            type='TransFusionBBoxCoder',
            voxel_size=[
                0.17,
                0.17,
            ]),
        bn_momentum=0.1,
        class_names=[
            'car',
            'truck',
            'bus',
            'bicycle',
            'pedestrian',
        ],
        common_heads=dict(
            center=[
                2,
                2,
            ],
            dim=[
                3,
                2,
            ],
            height=[
                1,
                2,
            ],
            rot=[
                2,
                2,
            ],
            vel=[
                2,
                2,
            ]),
        decoder_layer=dict(
            cross_attn_cfg=dict(dropout=0.1, embed_dims=128, num_heads=8),
            ffn_cfg=dict(
                act_cfg=dict(inplace=True, type='ReLU'),
                embed_dims=128,
                feedforward_channels=256,
                ffn_drop=0.1,
                num_fcs=2),
            norm_cfg=dict(type='LN'),
            pos_encoding_cfg=dict(input_channel=2, num_pos_feats=128),
            self_attn_cfg=dict(dropout=0.1, embed_dims=128, num_heads=8),
            type='TransformerDecoderLayer'),
        dense_heatmap_pooling_classes=[
            'car',
            'truck',
            'bus',
            'bicycle',
        ],
        hidden_channel=128,
        in_channels=128,
        loss_bbox=dict(
            loss_weight=0.25, reduction='mean', type='mmdet.L1Loss'),
        loss_cls=dict(
            alpha=0.25,
            gamma=2.0,
            loss_weight=1.0,
            reduction='mean',
            type='mmdet.FocalLoss',
            use_sigmoid=True),
        loss_heatmap=dict(
            loss_weight=1.0, reduction='mean', type='mmdet.GaussianFocalLoss'),
        nms_kernel_size=3,
        num_decoder_layers=1,
        num_proposals=500,
        test_cfg=dict(
            dataset='t4datasets',
            grid_size=[
                1440,
                1440,
                41,
            ],
            nms_clusters=[
                dict(class_names=[
                    'car',
                    'truck',
                    'bus',
                ], nms_threshold=0.5),
                dict(class_names=[
                    'bicycle',
                ], nms_threshold=0.5),
                dict(class_names=[
                    'pedestrian',
                ], nms_threshold=0.175),
            ],
            nms_type=None,
            out_size_factor=8,
            pc_range=[
                -122.4,
                -122.4,
            ],
            voxel_size=[
                0.17,
                0.17,
            ]),
        train_cfg=dict(
            assigner=dict(
                cls_cost=dict(
                    alpha=0.25,
                    gamma=2.0,
                    type='mmdet.FocalLossCost',
                    weight=0.15),
                iou_calculator=dict(coordinate='lidar', type='BboxOverlaps3D'),
                iou_cost=dict(type='IoU3DCost', weight=0.25),
                reg_cost=dict(type='BBoxBEVL1Cost', weight=0.25),
                type='HungarianAssigner3D'),
            code_weights=[
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                0.2,
                0.2,
            ],
            dataset='t4datasets',
            gaussian_overlap=0.1,
            grid_size=[
                1440,
                1440,
                41,
            ],
            min_radius=2,
            out_size_factor=8,
            point_cloud_range=[
                -122.4,
                -122.4,
                -3.0,
                122.4,
                122.4,
                5.0,
            ],
            pos_weight=-1,
            voxel_size=[
                0.17,
                0.17,
                0.2,
            ]),
        type='BEVFusionHead'),
    data_preprocessor=dict(
        bgr_to_rgb=False,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_size_divisor=32,
        rgb_to_bgr=False,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='Det3DDataPreprocessor'),
    img_aux_bbox_head_weight=0.3,
    img_backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            6,
            2,
        ],
        drop_path_rate=0.2,
        drop_rate=0.0,
        embed_dims=96,
        init_cfg=dict(
            checkpoint=
            'work_dirs/swin_transformer/swint_nuimages_pretrained.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            3,
            6,
            12,
            24,
        ],
        out_indices=[
            1,
            2,
            3,
        ],
        patch_norm=True,
        qk_scale=None,
        qkv_bias=True,
        type='mmdet.SwinTransformer',
        window_size=7,
        with_cp=False),
    img_neck=dict(
        act_cfg=dict(inplace=True, type='ReLU'),
        in_channels=[
            192,
            384,
            768,
        ],
        norm_cfg=dict(requires_grad=True, type='BN2d'),
        num_outs=3,
        out_channels=256,
        start_level=0,
        type='GeneralizedLSSFPN',
        upsample_cfg=dict(align_corners=False, mode='bilinear')),
    pts_backbone=None,
    pts_middle_encoder=None,
    pts_neck=None,
    pts_voxel_encoder=dict(num_features=5, type='HardSimpleVFE'),
    type='BEVFusion',
    view_transform=dict(
        dbound=[
            1.0,
            130,
            1.0,
        ],
        downsample=2,
        feature_size=[
            60,
            80,
        ],
        image_size=[
            480,
            640,
        ],
        in_channels=256,
        out_channels=128,
        type='NonLinearLSSTransform',
        xbound=[
            -122.4,
            122.4,
            0.68,
        ],
        ybound=[
            -122.4,
            122.4,
            0.68,
        ],
        zbound=[
            -10.0,
            10.0,
            20.0,
        ]))
name_mapping = dict({
    'ambulance': 'car',
    'animal': 'animal',
    'bicycle': 'bicycle',
    'bus': 'bus',
    'car': 'car',
    'construction_vehicle': 'truck',
    'construction_worker': 'pedestrian',
    'fire_truck': 'truck',
    'forklift': 'car',
    'kart': 'car',
    'motorcycle': 'bicycle',
    'movable_object.barrier': 'barrier',
    'movable_object.debris': 'debris',
    'movable_object.pushable_pullable': 'pushable_pullable',
    'movable_object.traffic_cone': 'traffic_cone',
    'movable_object.trafficcone': 'traffic_cone',
    'pedestrian': 'pedestrian',
    'pedestrian.adult': 'pedestrian',
    'pedestrian.child': 'pedestrian',
    'pedestrian.construction_worker': 'pedestrian',
    'pedestrian.personal_mobility': 'pedestrian',
    'pedestrian.police_officer': 'pedestrian',
    'pedestrian.stroller': 'pedestrian',
    'pedestrian.wheelchair': 'pedestrian',
    'personal_mobility': 'pedestrian',
    'police_car': 'car',
    'police_officer': 'pedestrian',
    'semi_trailer': 'trailer',
    'static_object.bicycle rack': 'bicycle rack',
    'static_object.bicycle_rack': 'bicycle_rack',
    'static_object.bollard': 'bollard',
    'stroller': 'pedestrian',
    'tractor_unit': 'truck',
    'trailer': 'trailer',
    'truck': 'truck',
    'vehicle.ambulance': 'car',
    'vehicle.bicycle': 'bicycle',
    'vehicle.bus': 'bus',
    'vehicle.bus (bendy & rigid)': 'bus',
    'vehicle.car': 'car',
    'vehicle.construction': 'truck',
    'vehicle.emergency (ambulance & police)': 'car',
    'vehicle.fire': 'truck',
    'vehicle.motorcycle': 'bicycle',
    'vehicle.police': 'car',
    'vehicle.trailer': 'trailer',
    'vehicle.truck': 'truck',
    'wheelchair': 'pedestrian'
})
num_class = 5
num_proposals = 500
num_workers = 32
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.1, norm_type=2),
    optimizer=dict(lr=0.0001, type='AdamW', weight_decay=0.01),
    type='OptimWrapper')
out_size_factor = 8
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        end=5,
        start_factor=0.3333333333333333,
        type='LinearLR'),
    dict(
        T_max=95,
        begin=5,
        by_epoch=True,
        convert_to_iter_based=True,
        end=100,
        eta_min=1e-08,
        type='CosineAnnealingLR'),
    dict(
        T_max=5,
        begin=0,
        by_epoch=True,
        convert_to_iter_based=True,
        end=5,
        eta_min=0.8947368421052632,
        type='CosineAnnealingMomentum'),
    dict(
        T_max=95,
        begin=5,
        by_epoch=True,
        convert_to_iter_based=True,
        end=100,
        eta_min=1,
        type='CosineAnnealingMomentum'),
]
point_cloud_range = [
    -122.4,
    -122.4,
    -3.0,
    122.4,
    122.4,
    5.0,
]
point_intensity_dim = 3
point_load_dim = 5
point_use_dim = 5
randomness = dict(deterministic=False, diff_rank_seed=False, seed=0)
resume = False
sweeps_num = 1
sync_bn = 'torch'
t_max = 5
test_batch_size = 2
test_cfg = dict()
test_dataloader = dict(
    batch_size=2,
    dataset=dict(
        ann_file='info/kokseang_2_3/t4dataset_j6gen2_base_infos_test.pkl',
        backend_args=None,
        box_type_3d='LiDAR',
        class_names=[
            'car',
            'truck',
            'bus',
            'bicycle',
            'pedestrian',
        ],
        data_prefix=dict(
            CAM_BACK='',
            CAM_BACK_LEFT='',
            CAM_BACK_RIGHT='',
            CAM_FRONT='',
            CAM_FRONT_LEFT='',
            CAM_FRONT_RIGHT='',
            pts='',
            sweeps=''),
        data_root='data/t4datasets/',
        metainfo=dict(classes=[
            'car',
            'truck',
            'bus',
            'bicycle',
            'pedestrian',
        ]),
        modality=dict(use_camera=True, use_lidar=True),
        pipeline=[
            dict(
                backend_args=None,
                camera_order=[
                    'CAM_FRONT',
                    'CAM_FRONT_LEFT',
                    'CAM_FRONT_RIGHT',
                    'CAM_BACK_LEFT',
                    'CAM_BACK_RIGHT',
                ],
                color_type='color',
                to_float32=True,
                type='BEVLoadMultiViewImageFromFiles'),
            dict(
                bot_pct_lim=[
                    0.0,
                    0.0,
                ],
                final_dim=[
                    480,
                    640,
                ],
                is_train=False,
                rand_flip=False,
                resize_lim=0.0,
                rot_lim=[
                    0.0,
                    0.0,
                ],
                type='ImageAug3D'),
            dict(
                keys=[
                    'img',
                    'gt_bboxes_3d',
                    'gt_labels_3d',
                ],
                meta_keys=[
                    'cam2img',
                    'ori_cam2img',
                    'lidar2cam',
                    'lidar2img',
                    'cam2lidar',
                    'ori_lidar2img',
                    'img_aug_matrix',
                    'box_type_3d',
                    'sample_idx',
                    'lidar_path',
                    'img_path',
                    'num_pts_feats',
                    'num_views',
                ],
                type='Pack3DDetInputs'),
        ],
        test_mode=True,
        type='T4Dataset'),
    num_workers=32,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    ann_file=
    'data/t4datasets/info/kokseang_2_3/t4dataset_j6gen2_base_infos_test.pkl',
    backend_args=None,
    class_names=[
        'car',
        'truck',
        'bus',
        'bicycle',
        'pedestrian',
    ],
    data_root='data/t4datasets/',
    eval_class_range=dict(
        bicycle=120, bus=120, car=120, pedestrian=120, truck=120),
    filter_attributes=[
        (
            'vehicle.bicycle',
            'vehicle_state.parked',
        ),
        (
            'vehicle.bicycle',
            'cycle_state.without_rider',
        ),
        (
            'vehicle.bicycle',
            'motorcycle_state.without_rider',
        ),
        (
            'vehicle.motorcycle',
            'vehicle_state.parked',
        ),
        (
            'vehicle.motorcycle',
            'cycle_state.without_rider',
        ),
        (
            'vehicle.motorcycle',
            'motorcycle_state.without_rider',
        ),
        (
            'bicycle',
            'vehicle_state.parked',
        ),
        (
            'bicycle',
            'cycle_state.without_rider',
        ),
        (
            'bicycle',
            'motorcycle_state.without_rider',
        ),
        (
            'motorcycle',
            'vehicle_state.parked',
        ),
        (
            'motorcycle',
            'cycle_state.without_rider',
        ),
        (
            'motorcycle',
            'motorcycle_state.without_rider',
        ),
    ],
    metric='bbox',
    name_mapping=dict({
        'ambulance': 'car',
        'animal': 'animal',
        'bicycle': 'bicycle',
        'bus': 'bus',
        'car': 'car',
        'construction_vehicle': 'truck',
        'construction_worker': 'pedestrian',
        'fire_truck': 'truck',
        'forklift': 'car',
        'kart': 'car',
        'motorcycle': 'bicycle',
        'movable_object.barrier': 'barrier',
        'movable_object.debris': 'debris',
        'movable_object.pushable_pullable': 'pushable_pullable',
        'movable_object.traffic_cone': 'traffic_cone',
        'movable_object.trafficcone': 'traffic_cone',
        'pedestrian': 'pedestrian',
        'pedestrian.adult': 'pedestrian',
        'pedestrian.child': 'pedestrian',
        'pedestrian.construction_worker': 'pedestrian',
        'pedestrian.personal_mobility': 'pedestrian',
        'pedestrian.police_officer': 'pedestrian',
        'pedestrian.stroller': 'pedestrian',
        'pedestrian.wheelchair': 'pedestrian',
        'personal_mobility': 'pedestrian',
        'police_car': 'car',
        'police_officer': 'pedestrian',
        'semi_trailer': 'trailer',
        'static_object.bicycle rack': 'bicycle rack',
        'static_object.bicycle_rack': 'bicycle_rack',
        'static_object.bollard': 'bollard',
        'stroller': 'pedestrian',
        'tractor_unit': 'truck',
        'trailer': 'trailer',
        'truck': 'truck',
        'vehicle.ambulance': 'car',
        'vehicle.bicycle': 'bicycle',
        'vehicle.bus': 'bus',
        'vehicle.bus (bendy & rigid)': 'bus',
        'vehicle.car': 'car',
        'vehicle.construction': 'truck',
        'vehicle.emergency (ambulance & police)': 'car',
        'vehicle.fire': 'truck',
        'vehicle.motorcycle': 'bicycle',
        'vehicle.police': 'car',
        'vehicle.trailer': 'trailer',
        'vehicle.truck': 'truck',
        'wheelchair': 'pedestrian'
    }),
    save_csv=True,
    type='T4Metric')
test_pipeline = [
    dict(
        backend_args=None,
        camera_order=[
            'CAM_FRONT',
            'CAM_FRONT_LEFT',
            'CAM_FRONT_RIGHT',
            'CAM_BACK_LEFT',
            'CAM_BACK_RIGHT',
        ],
        color_type='color',
        to_float32=True,
        type='BEVLoadMultiViewImageFromFiles'),
    dict(
        bot_pct_lim=[
            0.0,
            0.0,
        ],
        final_dim=[
            480,
            640,
        ],
        is_train=False,
        rand_flip=False,
        resize_lim=0.0,
        rot_lim=[
            0.0,
            0.0,
        ],
        type='ImageAug3D'),
    dict(
        keys=[
            'img',
            'gt_bboxes_3d',
            'gt_labels_3d',
        ],
        meta_keys=[
            'cam2img',
            'ori_cam2img',
            'lidar2cam',
            'lidar2img',
            'cam2lidar',
            'ori_lidar2img',
            'img_aug_matrix',
            'box_type_3d',
            'sample_idx',
            'lidar_path',
            'img_path',
            'num_pts_feats',
            'num_views',
        ],
        type='Pack3DDetInputs'),
]
train_batch_size = 8
train_cfg = dict(
    by_epoch=True,
    dynamic_intervals=[
        (
            95,
            2,
        ),
    ],
    max_epochs=100,
    val_interval=10)
train_dataloader = dict(
    batch_size=8,
    dataset=dict(
        ann_file='info/kokseang_2_3/t4dataset_j6gen2_base_infos_train.pkl',
        backend_args=None,
        box_type_3d='LiDAR',
        class_names=[
            'car',
            'truck',
            'bus',
            'bicycle',
            'pedestrian',
        ],
        data_prefix=dict(
            CAM_BACK='',
            CAM_BACK_LEFT='',
            CAM_BACK_RIGHT='',
            CAM_FRONT='',
            CAM_FRONT_LEFT='',
            CAM_FRONT_RIGHT='',
            pts='',
            sweeps=''),
        data_root='data/t4datasets/',
        filter_cfg=dict(filter_frames_with_camera_order=[
            'CAM_FRONT',
            'CAM_FRONT_LEFT',
            'CAM_FRONT_RIGHT',
            'CAM_BACK_LEFT',
            'CAM_BACK_RIGHT',
        ]),
        metainfo=dict(classes=[
            'car',
            'truck',
            'bus',
            'bicycle',
            'pedestrian',
        ]),
        modality=dict(use_camera=True, use_lidar=True),
        pipeline=[
            dict(
                backend_args=None,
                camera_order=[
                    'CAM_FRONT',
                    'CAM_FRONT_LEFT',
                    'CAM_FRONT_RIGHT',
                    'CAM_BACK_LEFT',
                    'CAM_BACK_RIGHT',
                ],
                color_type='color',
                to_float32=True,
                type='BEVLoadMultiViewImageFromFiles'),
            dict(
                type='LoadAnnotations3D',
                with_attr_label=False,
                with_bbox_3d=True,
                with_label_3d=True),
            dict(
                bot_pct_lim=[
                    0.0,
                    0.0,
                ],
                final_dim=[
                    480,
                    640,
                ],
                is_train=True,
                rand_flip=True,
                resize_lim=0.02,
                rot_lim=[
                    0.0,
                    0.0,
                ],
                type='ImageAug3D'),
            dict(
                point_cloud_range=[
                    -122.4,
                    -122.4,
                    -3.0,
                    122.4,
                    122.4,
                    5.0,
                ],
                type='ObjectRangeFilter'),
            dict(
                classes=[
                    'car',
                    'truck',
                    'construction_vehicle',
                    'bus',
                    'trailer',
                    'barrier',
                    'motorcycle',
                    'bicycle',
                    'pedestrian',
                    'traffic_cone',
                ],
                type='ObjectNameFilter'),
            dict(
                keys=[
                    'img',
                    'gt_bboxes_3d',
                    'gt_labels_3d',
                    'gt_bboxes',
                    'gt_labels',
                ],
                meta_keys=[
                    'cam2img',
                    'ori_cam2img',
                    'lidar2cam',
                    'lidar2img',
                    'cam2lidar',
                    'ori_lidar2img',
                    'img_aug_matrix',
                    'box_type_3d',
                    'sample_idx',
                    'lidar_path',
                    'img_path',
                    'transformation_3d_flow',
                    'pcd_rotation',
                    'pcd_scale_factor',
                    'pcd_trans',
                    'lidar_aug_matrix',
                ],
                type='Pack3DDetInputs'),
        ],
        test_mode=False,
        type='T4Dataset'),
    num_workers=32,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_gpu_size = 2
train_pipeline = [
    dict(
        backend_args=None,
        camera_order=[
            'CAM_FRONT',
            'CAM_FRONT_LEFT',
            'CAM_FRONT_RIGHT',
            'CAM_BACK_LEFT',
            'CAM_BACK_RIGHT',
        ],
        color_type='color',
        to_float32=True,
        type='BEVLoadMultiViewImageFromFiles'),
    dict(
        type='LoadAnnotations3D',
        with_attr_label=False,
        with_bbox_3d=True,
        with_label_3d=True),
    dict(
        bot_pct_lim=[
            0.0,
            0.0,
        ],
        final_dim=[
            480,
            640,
        ],
        is_train=True,
        rand_flip=True,
        resize_lim=0.02,
        rot_lim=[
            0.0,
            0.0,
        ],
        type='ImageAug3D'),
    dict(
        point_cloud_range=[
            -122.4,
            -122.4,
            -3.0,
            122.4,
            122.4,
            5.0,
        ],
        type='ObjectRangeFilter'),
    dict(
        classes=[
            'car',
            'truck',
            'construction_vehicle',
            'bus',
            'trailer',
            'barrier',
            'motorcycle',
            'bicycle',
            'pedestrian',
            'traffic_cone',
        ],
        type='ObjectNameFilter'),
    dict(
        keys=[
            'img',
            'gt_bboxes_3d',
            'gt_labels_3d',
            'gt_bboxes',
            'gt_labels',
        ],
        meta_keys=[
            'cam2img',
            'ori_cam2img',
            'lidar2cam',
            'lidar2img',
            'cam2lidar',
            'ori_lidar2img',
            'img_aug_matrix',
            'box_type_3d',
            'sample_idx',
            'lidar_path',
            'img_path',
            'transformation_3d_flow',
            'pcd_rotation',
            'pcd_scale_factor',
            'pcd_trans',
            'lidar_aug_matrix',
        ],
        type='Pack3DDetInputs'),
]
val_cfg = dict()
val_dataloader = dict(
    batch_size=2,
    dataset=dict(
        ann_file='info/kokseang_2_3/t4dataset_j6gen2_base_infos_val.pkl',
        backend_args=None,
        box_type_3d='LiDAR',
        class_names=[
            'car',
            'truck',
            'bus',
            'bicycle',
            'pedestrian',
        ],
        data_prefix=dict(
            CAM_BACK='',
            CAM_BACK_LEFT='',
            CAM_BACK_RIGHT='',
            CAM_FRONT='',
            CAM_FRONT_LEFT='',
            CAM_FRONT_RIGHT='',
            pts='',
            sweeps=''),
        data_root='data/t4datasets/',
        metainfo=dict(classes=[
            'car',
            'truck',
            'bus',
            'bicycle',
            'pedestrian',
        ]),
        modality=dict(use_camera=True, use_lidar=True),
        pipeline=[
            dict(
                backend_args=None,
                camera_order=[
                    'CAM_FRONT',
                    'CAM_FRONT_LEFT',
                    'CAM_FRONT_RIGHT',
                    'CAM_BACK_LEFT',
                    'CAM_BACK_RIGHT',
                ],
                color_type='color',
                to_float32=True,
                type='BEVLoadMultiViewImageFromFiles'),
            dict(
                bot_pct_lim=[
                    0.0,
                    0.0,
                ],
                final_dim=[
                    480,
                    640,
                ],
                is_train=False,
                rand_flip=False,
                resize_lim=0.0,
                rot_lim=[
                    0.0,
                    0.0,
                ],
                type='ImageAug3D'),
            dict(
                keys=[
                    'img',
                    'gt_bboxes_3d',
                    'gt_labels_3d',
                ],
                meta_keys=[
                    'cam2img',
                    'ori_cam2img',
                    'lidar2cam',
                    'lidar2img',
                    'cam2lidar',
                    'ori_lidar2img',
                    'img_aug_matrix',
                    'box_type_3d',
                    'sample_idx',
                    'lidar_path',
                    'img_path',
                    'num_pts_feats',
                    'num_views',
                ],
                type='Pack3DDetInputs'),
        ],
        test_mode=True,
        type='T4Dataset'),
    num_workers=32,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    ann_file=
    'data/t4datasets/info/kokseang_2_3/t4dataset_j6gen2_base_infos_val.pkl',
    backend_args=None,
    class_names=[
        'car',
        'truck',
        'bus',
        'bicycle',
        'pedestrian',
    ],
    data_root='data/t4datasets/',
    eval_class_range=dict(
        bicycle=120, bus=120, car=120, pedestrian=120, truck=120),
    filter_attributes=[
        (
            'vehicle.bicycle',
            'vehicle_state.parked',
        ),
        (
            'vehicle.bicycle',
            'cycle_state.without_rider',
        ),
        (
            'vehicle.bicycle',
            'motorcycle_state.without_rider',
        ),
        (
            'vehicle.motorcycle',
            'vehicle_state.parked',
        ),
        (
            'vehicle.motorcycle',
            'cycle_state.without_rider',
        ),
        (
            'vehicle.motorcycle',
            'motorcycle_state.without_rider',
        ),
        (
            'bicycle',
            'vehicle_state.parked',
        ),
        (
            'bicycle',
            'cycle_state.without_rider',
        ),
        (
            'bicycle',
            'motorcycle_state.without_rider',
        ),
        (
            'motorcycle',
            'vehicle_state.parked',
        ),
        (
            'motorcycle',
            'cycle_state.without_rider',
        ),
        (
            'motorcycle',
            'motorcycle_state.without_rider',
        ),
    ],
    metric='bbox',
    name_mapping=dict({
        'ambulance': 'car',
        'animal': 'animal',
        'bicycle': 'bicycle',
        'bus': 'bus',
        'car': 'car',
        'construction_vehicle': 'truck',
        'construction_worker': 'pedestrian',
        'fire_truck': 'truck',
        'forklift': 'car',
        'kart': 'car',
        'motorcycle': 'bicycle',
        'movable_object.barrier': 'barrier',
        'movable_object.debris': 'debris',
        'movable_object.pushable_pullable': 'pushable_pullable',
        'movable_object.traffic_cone': 'traffic_cone',
        'movable_object.trafficcone': 'traffic_cone',
        'pedestrian': 'pedestrian',
        'pedestrian.adult': 'pedestrian',
        'pedestrian.child': 'pedestrian',
        'pedestrian.construction_worker': 'pedestrian',
        'pedestrian.personal_mobility': 'pedestrian',
        'pedestrian.police_officer': 'pedestrian',
        'pedestrian.stroller': 'pedestrian',
        'pedestrian.wheelchair': 'pedestrian',
        'personal_mobility': 'pedestrian',
        'police_car': 'car',
        'police_officer': 'pedestrian',
        'semi_trailer': 'trailer',
        'static_object.bicycle rack': 'bicycle rack',
        'static_object.bicycle_rack': 'bicycle_rack',
        'static_object.bollard': 'bollard',
        'stroller': 'pedestrian',
        'tractor_unit': 'truck',
        'trailer': 'trailer',
        'truck': 'truck',
        'vehicle.ambulance': 'car',
        'vehicle.bicycle': 'bicycle',
        'vehicle.bus': 'bus',
        'vehicle.bus (bendy & rigid)': 'bus',
        'vehicle.car': 'car',
        'vehicle.construction': 'truck',
        'vehicle.emergency (ambulance & police)': 'car',
        'vehicle.fire': 'truck',
        'vehicle.motorcycle': 'bicycle',
        'vehicle.police': 'car',
        'vehicle.trailer': 'trailer',
        'vehicle.truck': 'truck',
        'wheelchair': 'pedestrian'
    }),
    type='T4Metric')
val_interval = 10
vis_backends = [
    dict(type='LocalVisBackend'),
    dict(type='TensorboardVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='Det3DLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
        dict(type='TensorboardVisBackend'),
    ])
voxel_size = [
    0.17,
    0.17,
    0.2,
]
work_dir = 'work_dirs/bevfusion_camera_4xb8_j6gen2_base_swin_bn_lr_1e-4_128c_downsample_e100_transfusion_linearlr_120m_grad_01/'

11/12 11:16:59 - mmengine - INFO - Loads checkpoint by local backend from path: work_dirs/swin_transformer/swint_nuimages_pretrained.pth
11/12 11:16:59 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
11/12 11:16:59 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
(NORMAL      ) DisableObjectSampleHook            
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) Det3DVisualizationHook             
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) Det3DVisualizationHook             
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
11/12 11:17:04 - mmengine - INFO - ------------------------------
11/12 11:17:04 - mmengine - INFO - The length of test dataset: 2076
11/12 11:17:04 - mmengine - INFO - The number of instances per category in the dataset:
+------------+--------+
| category   | number |
+------------+--------+
| car        | 79609  |
| truck      | 6581   |
| bus        | 1788   |
| bicycle    | 1356   |
| pedestrian | 13137  |
+------------+--------+
11/12 11:17:04 - mmengine - INFO - Valid dataset instances: {'car': 63841, 'truck': 5834, 'bus': 1668, 'bicycle': 1259, 'pedestrian': 11530}
11/12 11:17:19 - mmengine - WARNING - Failed to search registry with scope "mmdet3d" in the "Codebases" registry tree. As a workaround, the current "Codebases" registry in "mmdeploy" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "mmdet3d" is a correct scope, or whether the registry is initialized.
11/12 11:17:19 - mmengine - WARNING - Failed to search registry with scope "mmdet3d" in the "mmdet3d_tasks" registry tree. As a workaround, the current "mmdet3d_tasks" registry in "mmdeploy" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "mmdet3d" is a correct scope, or whether the registry is initialized.
11/12 11:17:19 - mmengine - INFO - Loads checkpoint by local backend from path: work_dirs/swin_transformer/swint_nuimages_pretrained.pth
Loads checkpoint by local backend from path: work_dirs/bevfusion_camera_4xb8_j6gen2_base_swin_bn_lr_1e-4_128c_downsample_e100_transfusion_linearlr_120m_grad_01/epoch_80.pth
11/12 11:17:20 - mmengine - INFO - Export PyTorch model to ONNX: work_dirs/bevfusion_camera_4xb8_j6gen2_base_swin_bn_lr_1e-4_128c_downsample_e100_transfusion_linearlr_120m_grad_01/camera_bev_network.onnx.
11/12 11:17:20 - mmengine - WARNING - Can not find torch.nn.functional._scaled_dot_product_attention, function rewrite will not be applied
Torch IR graph at exception: graph(%0 : Float(5, 4, 4, strides=[16, 4, 1], requires_grad=0, device=cuda:0),
      %1 : Float(5, 4, 4, strides=[16, 4, 1], requires_grad=0, device=cuda:0),
      %geom_feats : Float(1395622, 4, strides=[4, 1], requires_grad=0, device=cuda:0),
      %kept.1 : Bool(3096000, strides=[1], requires_grad=0, device=cuda:0),
      %ranks : Long(1395622, strides=[1], requires_grad=0, device=cuda:0),
      %indices : Long(1395622, strides=[1], requires_grad=0, device=cuda:0),
      %6 : Float(5, 256, 60, 80, strides=[1228800, 4800, 80, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.patch_embed.projection.weight : Float(96, 3, 4, 4, strides=[48, 16, 4, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.patch_embed.projection.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.patch_embed.norm.weight : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.patch_embed.norm.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.norm1.weight : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.norm1.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table : Float(169, 3, strides=[3, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.attn.w_msa.qkv.weight : Float(288, 96, strides=[96, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.attn.w_msa.qkv.bias : Float(288, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.attn.w_msa.proj.weight : Float(96, 96, strides=[96, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.attn.w_msa.proj.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.norm2.weight : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.norm2.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.ffn.layers.0.0.weight : Float(384, 96, strides=[96, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.ffn.layers.0.0.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.ffn.layers.1.weight : Float(96, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.0.ffn.layers.1.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.norm1.weight : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.norm1.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table : Float(169, 3, strides=[3, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.attn.w_msa.qkv.weight : Float(288, 96, strides=[96, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.attn.w_msa.qkv.bias : Float(288, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.attn.w_msa.proj.weight : Float(96, 96, strides=[96, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.attn.w_msa.proj.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.norm2.weight : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.norm2.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.ffn.layers.0.0.weight : Float(384, 96, strides=[96, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.ffn.layers.0.0.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.ffn.layers.1.weight : Float(96, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.blocks.1.ffn.layers.1.bias : Float(96, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.downsample.norm.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.downsample.norm.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.0.downsample.reduction.weight : Float(192, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.norm1.weight : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.norm1.bias : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table : Float(169, 6, strides=[6, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.attn.w_msa.qkv.weight : Float(576, 192, strides=[192, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.attn.w_msa.qkv.bias : Float(576, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.attn.w_msa.proj.weight : Float(192, 192, strides=[192, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.attn.w_msa.proj.bias : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.norm2.weight : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.norm2.bias : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.ffn.layers.0.0.weight : Float(768, 192, strides=[192, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.ffn.layers.0.0.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.ffn.layers.1.weight : Float(192, 768, strides=[768, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.0.ffn.layers.1.bias : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.norm1.weight : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.norm1.bias : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table : Float(169, 6, strides=[6, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.attn.w_msa.qkv.weight : Float(576, 192, strides=[192, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.attn.w_msa.qkv.bias : Float(576, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.attn.w_msa.proj.weight : Float(192, 192, strides=[192, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.attn.w_msa.proj.bias : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.norm2.weight : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.norm2.bias : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.ffn.layers.0.0.weight : Float(768, 192, strides=[192, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.ffn.layers.0.0.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.ffn.layers.1.weight : Float(192, 768, strides=[768, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.blocks.1.ffn.layers.1.bias : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.downsample.norm.weight : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.downsample.norm.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.1.downsample.reduction.weight : Float(384, 768, strides=[768, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.norm1.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.norm1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table : Float(169, 12, strides=[12, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.attn.w_msa.qkv.weight : Float(1152, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.attn.w_msa.qkv.bias : Float(1152, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.attn.w_msa.proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.attn.w_msa.proj.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.norm2.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.norm2.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.ffn.layers.0.0.weight : Float(1536, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.ffn.layers.0.0.bias : Float(1536, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.ffn.layers.1.weight : Float(384, 1536, strides=[1536, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.0.ffn.layers.1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.norm1.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.norm1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table : Float(169, 12, strides=[12, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.attn.w_msa.qkv.weight : Float(1152, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.attn.w_msa.qkv.bias : Float(1152, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.attn.w_msa.proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.attn.w_msa.proj.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.norm2.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.norm2.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.ffn.layers.0.0.weight : Float(1536, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.ffn.layers.0.0.bias : Float(1536, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.ffn.layers.1.weight : Float(384, 1536, strides=[1536, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.1.ffn.layers.1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.norm1.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.norm1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table : Float(169, 12, strides=[12, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.attn.w_msa.qkv.weight : Float(1152, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.attn.w_msa.qkv.bias : Float(1152, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.attn.w_msa.proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.attn.w_msa.proj.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.norm2.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.norm2.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.ffn.layers.0.0.weight : Float(1536, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.ffn.layers.0.0.bias : Float(1536, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.ffn.layers.1.weight : Float(384, 1536, strides=[1536, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.2.ffn.layers.1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.norm1.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.norm1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table : Float(169, 12, strides=[12, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.attn.w_msa.qkv.weight : Float(1152, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.attn.w_msa.qkv.bias : Float(1152, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.attn.w_msa.proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.attn.w_msa.proj.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.norm2.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.norm2.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.ffn.layers.0.0.weight : Float(1536, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.ffn.layers.0.0.bias : Float(1536, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.ffn.layers.1.weight : Float(384, 1536, strides=[1536, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.3.ffn.layers.1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.norm1.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.norm1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table : Float(169, 12, strides=[12, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.attn.w_msa.qkv.weight : Float(1152, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.attn.w_msa.qkv.bias : Float(1152, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.attn.w_msa.proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.attn.w_msa.proj.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.norm2.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.norm2.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.ffn.layers.0.0.weight : Float(1536, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.ffn.layers.0.0.bias : Float(1536, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.ffn.layers.1.weight : Float(384, 1536, strides=[1536, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.4.ffn.layers.1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.norm1.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.norm1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table : Float(169, 12, strides=[12, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.attn.w_msa.qkv.weight : Float(1152, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.attn.w_msa.qkv.bias : Float(1152, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.attn.w_msa.proj.weight : Float(384, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.attn.w_msa.proj.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.norm2.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.norm2.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.ffn.layers.0.0.weight : Float(1536, 384, strides=[384, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.ffn.layers.0.0.bias : Float(1536, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.ffn.layers.1.weight : Float(384, 1536, strides=[1536, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.blocks.5.ffn.layers.1.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.downsample.norm.weight : Float(1536, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.downsample.norm.bias : Float(1536, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.2.downsample.reduction.weight : Float(768, 1536, strides=[1536, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.norm1.weight : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.norm1.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table : Float(169, 24, strides=[24, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.attn.w_msa.qkv.weight : Float(2304, 768, strides=[768, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.attn.w_msa.qkv.bias : Float(2304, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.attn.w_msa.proj.weight : Float(768, 768, strides=[768, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.attn.w_msa.proj.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.norm2.weight : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.norm2.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.ffn.layers.0.0.weight : Float(3072, 768, strides=[768, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.ffn.layers.0.0.bias : Float(3072, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.ffn.layers.1.weight : Float(768, 3072, strides=[3072, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.0.ffn.layers.1.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.norm1.weight : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.norm1.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table : Float(169, 24, strides=[24, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.attn.w_msa.relative_position_index : Long(49, 49, strides=[49, 1], requires_grad=0, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.attn.w_msa.qkv.weight : Float(2304, 768, strides=[768, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.attn.w_msa.qkv.bias : Float(2304, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.attn.w_msa.proj.weight : Float(768, 768, strides=[768, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.attn.w_msa.proj.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.norm2.weight : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.norm2.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.ffn.layers.0.0.weight : Float(3072, 768, strides=[768, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.ffn.layers.0.0.bias : Float(3072, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.ffn.layers.1.weight : Float(768, 3072, strides=[3072, 1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.stages.3.blocks.1.ffn.layers.1.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.norm1.weight : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.norm1.bias : Float(192, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.norm2.weight : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.norm2.bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.norm3.weight : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_backbone.norm3.bias : Float(768, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_neck.lateral_convs.0.conv.weight : Float(256, 448, 1, 1, strides=[448, 1, 1, 1], requires_grad=1, device=cuda:0),
      %mod.img_neck.lateral_convs.0.bn.weight : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_neck.lateral_convs.0.bn.bias : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_neck.lateral_convs.0.bn.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.img_neck.lateral_convs.0.bn.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.img_neck.lateral_convs.0.bn.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.img_neck.lateral_convs.1.conv.weight : Float(256, 1152, 1, 1, strides=[1152, 1, 1, 1], requires_grad=1, device=cuda:0),
      %mod.img_neck.lateral_convs.1.bn.weight : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_neck.lateral_convs.1.bn.bias : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_neck.lateral_convs.1.bn.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.img_neck.lateral_convs.1.bn.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.img_neck.lateral_convs.1.bn.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.img_neck.fpn_convs.0.conv.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=1, device=cuda:0),
      %mod.img_neck.fpn_convs.0.bn.weight : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_neck.fpn_convs.0.bn.bias : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_neck.fpn_convs.0.bn.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.img_neck.fpn_convs.0.bn.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.img_neck.fpn_convs.0.bn.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.img_neck.fpn_convs.1.conv.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=1, device=cuda:0),
      %mod.img_neck.fpn_convs.1.bn.weight : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_neck.fpn_convs.1.bn.bias : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.img_neck.fpn_convs.1.bn.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.img_neck.fpn_convs.1.bn.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.img_neck.fpn_convs.1.bn.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.view_transform.dx : Float(3, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.bx : Float(3, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.nx : Long(3, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.frustum : Float(129, 60, 80, 3, strides=[14400, 240, 3, 1], requires_grad=0, device=cuda:0),
      %mod.view_transform.depthnet.net.0.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=1, device=cuda:0),
      %mod.view_transform.depthnet.net.0.bias : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.depthnet.net.1.weight : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.depthnet.net.1.bias : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.depthnet.net.1.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.depthnet.net.1.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.depthnet.net.1.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.view_transform.depthnet.net.3.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=1, device=cuda:0),
      %mod.view_transform.depthnet.net.3.bias : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.depthnet.net.4.weight : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.depthnet.net.4.bias : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.depthnet.net.4.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.depthnet.net.4.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.depthnet.net.4.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.view_transform.depthnet.net.6.weight : Float(257, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=1, device=cuda:0),
      %mod.view_transform.depthnet.net.6.bias : Float(257, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.downsample.net.0.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=1, device=cuda:0),
      %mod.view_transform.downsample.net.1.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.downsample.net.1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.downsample.net.1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.downsample.net.1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.downsample.net.1.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.view_transform.downsample.net.3.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=1, device=cuda:0),
      %mod.view_transform.downsample.net.4.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.downsample.net.4.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.downsample.net.4.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.downsample.net.4.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.downsample.net.4.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.view_transform.downsample.net.6.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=1, device=cuda:0),
      %mod.view_transform.downsample.net.7.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.downsample.net.7.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.view_transform.downsample.net.7.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.downsample.net.7.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.view_transform.downsample.net.7.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.bbox_head.shared_conv.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.shared_conv.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.heatmap_head.0.conv.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.heatmap_head.0.bn.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.heatmap_head.0.bn.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.heatmap_head.0.bn.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.heatmap_head.0.bn.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.heatmap_head.0.bn.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.bbox_head.heatmap_head.1.weight : Float(5, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.heatmap_head.1.bias : Float(5, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.class_encoding.weight : Float(128, 5, 1, strides=[5, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.class_encoding.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.self_attn.attn.in_proj_weight : Float(384, 128, strides=[128, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.self_attn.attn.in_proj_bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.self_attn.attn.out_proj.weight : Float(128, 128, strides=[128, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.self_attn.attn.out_proj.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_attn.attn.in_proj_weight : Float(384, 128, strides=[128, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_attn.attn.in_proj_bias : Float(384, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_attn.attn.out_proj.weight : Float(128, 128, strides=[128, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_attn.attn.out_proj.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.ffn.layers.0.0.weight : Float(256, 128, strides=[128, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.ffn.layers.0.0.bias : Float(256, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.ffn.layers.1.weight : Float(128, 256, strides=[256, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.ffn.layers.1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.norms.0.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.norms.0.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.norms.1.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.norms.1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.norms.2.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.norms.2.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.0.weight : Float(128, 2, 1, strides=[2, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.0.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.1.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.1.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.3.weight : Float(128, 128, 1, strides=[128, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.3.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.0.weight : Float(128, 2, 1, strides=[2, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.0.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.1.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.1.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.3.weight : Float(128, 128, 1, strides=[128, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.3.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.center.0.conv.weight : Float(64, 128, 1, strides=[128, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.center.0.bn.weight : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.center.0.bn.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.center.0.bn.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.center.0.bn.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.center.0.bn.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.center.1.weight : Float(2, 64, 1, strides=[64, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.center.1.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.height.0.conv.weight : Float(64, 128, 1, strides=[128, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.height.0.bn.weight : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.height.0.bn.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.height.0.bn.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.height.0.bn.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.height.0.bn.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.height.1.weight : Float(1, 64, 1, strides=[64, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.height.1.bias : Float(1, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.dim.0.conv.weight : Float(64, 128, 1, strides=[128, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.dim.0.bn.weight : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.dim.0.bn.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.dim.0.bn.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.dim.0.bn.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.dim.0.bn.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.dim.1.weight : Float(3, 64, 1, strides=[64, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.dim.1.bias : Float(3, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.rot.0.conv.weight : Float(64, 128, 1, strides=[128, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.rot.0.bn.weight : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.rot.0.bn.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.rot.0.bn.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.rot.0.bn.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.rot.0.bn.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.rot.1.weight : Float(2, 64, 1, strides=[64, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.rot.1.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.vel.0.conv.weight : Float(64, 128, 1, strides=[128, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.vel.0.bn.weight : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.vel.0.bn.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.vel.0.bn.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.vel.0.bn.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.vel.0.bn.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.vel.1.weight : Float(2, 64, 1, strides=[64, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.vel.1.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.heatmap.0.conv.weight : Float(64, 128, 1, strides=[128, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.heatmap.0.bn.weight : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.heatmap.0.bn.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.heatmap.0.bn.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.heatmap.0.bn.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.heatmap.0.bn.num_batches_tracked : Long(requires_grad=0, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.heatmap.1.weight : Float(5, 64, 1, strides=[64, 1, 1], requires_grad=1, device=cuda:0),
      %mod.bbox_head.prediction_heads.0.heatmap.1.bias : Float(5, strides=[1], requires_grad=1, device=cuda:0)):
  %8515 : Long(device=cpu) = prim::Constant[value={0}](), scope: containers.TrtBevFusionCameraOnlyContainer::
  %img : Float(1, 5, 256, 60, 80, strides=[6144000, 1228800, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::unsqueeze(%6, %8515), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:121:0
  %6664 : Long(device=cpu) = aten::size(%img, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:671:0
  %8516 : Long(device=cpu) = prim::Constant[value={1}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6668 : Long(device=cpu) = aten::size(%img, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:671:0
  %8517 : Long(device=cpu) = prim::Constant[value={2}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6672 : Long(device=cpu) = aten::size(%img, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:671:0
  %8518 : Long(device=cpu) = prim::Constant[value={3}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6676 : Long(device=cpu) = aten::size(%img, %8518), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:671:0
  %8519 : Long(device=cpu) = prim::Constant[value={4}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6681 : Long(device=cpu) = aten::size(%img, %8519), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:671:0
  %6685 : Long(requires_grad=0, device=cpu) = aten::mul(%6664, %6668), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:673:0
  %6687 : int[] = prim::ListConstruct(%6685, %6672, %6676, %6681), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %input.1 : Float(5, 256, 60, 80, strides=[1228800, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::view(%img, %6687), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:673:0
  %8312 : int[] = prim::Constant[value=[1, 1]]()
  %8520 : Bool(device=cpu) = prim::Constant[value={0}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DepthLSSNet::depthnet/torch.nn.modules.container.Sequential::net/torch.nn.modules.conv.Conv2d::net.0
  %8315 : int[] = prim::Constant[value=[0, 0]]()
  %8521 : Bool(device=cpu) = prim::Constant[value={1}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DepthLSSNet::depthnet/torch.nn.modules.container.Sequential::net/torch.nn.modules.conv.Conv2d::net.0
  %input.3 : Float(5, 256, 60, 80, strides=[1228800, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.1, %mod.view_transform.depthnet.net.0.weight, %mod.view_transform.depthnet.net.0.bias, %8312, %8312, %8312, %8520, %8315, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DepthLSSNet::depthnet/torch.nn.modules.container.Sequential::net/torch.nn.modules.conv.Conv2d::net.0 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:543:0
  %8522 : Double(device=cpu) = prim::Constant[value={0.1}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DepthLSSNet::depthnet/torch.nn.modules.container.Sequential::net/torch.nn.modules.batchnorm.BatchNorm2d::net.1
  %8523 : Double(device=cpu) = prim::Constant[value={1e-05}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DepthLSSNet::depthnet/torch.nn.modules.container.Sequential::net/torch.nn.modules.batchnorm.BatchNorm2d::net.1
  %input.5 : Float(5, 256, 60, 80, strides=[1228800, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.3, %mod.view_transform.depthnet.net.1.weight, %mod.view_transform.depthnet.net.1.bias, %mod.view_transform.depthnet.net.1.running_mean, %mod.view_transform.depthnet.net.1.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DepthLSSNet::depthnet/torch.nn.modules.container.Sequential::net/torch.nn.modules.batchnorm.BatchNorm2d::net.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8473 : Float(5, 256, 60, 80, strides=[1228800, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.5), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DepthLSSNet::depthnet/torch.nn.modules.container.Sequential::net/torch.nn.modules.activation.ReLU::net.2 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %input.9 : Float(5, 256, 60, 80, strides=[1228800, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8473, %mod.view_transform.depthnet.net.3.weight, %mod.view_transform.depthnet.net.3.bias, %8312, %8312, %8312, %8520, %8315, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DepthLSSNet::depthnet/torch.nn.modules.container.Sequential::net/torch.nn.modules.conv.Conv2d::net.3 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:543:0
  %input.11 : Float(5, 256, 60, 80, strides=[1228800, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.9, %mod.view_transform.depthnet.net.4.weight, %mod.view_transform.depthnet.net.4.bias, %mod.view_transform.depthnet.net.4.running_mean, %mod.view_transform.depthnet.net.4.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DepthLSSNet::depthnet/torch.nn.modules.container.Sequential::net/torch.nn.modules.batchnorm.BatchNorm2d::net.4 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8474 : Float(5, 256, 60, 80, strides=[1228800, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.11), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DepthLSSNet::depthnet/torch.nn.modules.container.Sequential::net/torch.nn.modules.activation.ReLU::net.5 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %6757 : Float(5, 257, 60, 80, strides=[1233600, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8474, %mod.view_transform.depthnet.net.6.weight, %mod.view_transform.depthnet.net.6.bias, %8312, %8315, %8312, %8520, %8315, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DepthLSSNet::depthnet/torch.nn.modules.container.Sequential::net/torch.nn.modules.conv.Conv2d::net.6 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:543:0
  %8524 : Long(device=cpu) = prim::Constant[value={9223372036854775807}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6762 : Float(5, 257, 60, 80, strides=[1233600, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::slice(%6757, %8515, %8515, %8524, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:676:0
  %8525 : Long(device=cpu) = prim::Constant[value={129}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6767 : Float(5, 129, 60, 80, strides=[1233600, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::slice(%6762, %8516, %8515, %8525, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:676:0
  %6769 : NoneType = prim::Constant(), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6770 : Float(5, 129, 60, 80, strides=[619200, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::softmax(%6767, %8516, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:676:0
  %6772 : Float(5, 1, 129, 60, 80, strides=[619200, 619200, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::unsqueeze(%6770, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:677:0
  %8526 : Long(device=cpu) = prim::Constant[value={257}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6782 : Float(5, 128, 60, 80, strides=[1233600, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::slice(%6762, %8516, %8525, %8526, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:677:0
  %6784 : Float(5, 128, 1, 60, 80, strides=[1233600, 4800, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::unsqueeze(%6782, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:677:0
  %6785 : Float(5, 128, 129, 60, 80, strides=[79257600, 619200, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::mul(%6772, %6784), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:677:0
  %8527 : Long(device=cpu) = prim::Constant[value={128}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6788 : int[] = prim::ListConstruct(%6664, %6668, %8527, %8525, %6676, %6681), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6789 : Float(1, 5, 128, 129, 60, 80, strides=[396288000, 79257600, 619200, 4800, 80, 1], requires_grad=0, device=cuda:0) = aten::view(%6785, %6788), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:679:0
  %8324 : int[] = prim::Constant[value=[0, 1, 3, 4, 5, 2]]()
  %x.1 : Float(1, 5, 129, 60, 80, 128, strides=[396288000, 79257600, 4800, 80, 1, 619200], requires_grad=0, device=cuda:0) = aten::permute(%6789, %8324), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:680:0
  %6799 : Long(device=cpu) = aten::size(%x.1, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:291:0
  %6802 : Long(device=cpu) = aten::size(%x.1, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:291:0
  %6805 : Long(device=cpu) = aten::size(%x.1, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:291:0
  %6808 : Long(device=cpu) = aten::size(%x.1, %8518), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:291:0
  %6811 : Long(device=cpu) = aten::size(%x.1, %8519), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:291:0
  %8528 : Long(device=cpu) = prim::Constant[value={5}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6814 : Long(device=cpu) = aten::size(%x.1, %8528), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:291:0
  %6817 : Long(requires_grad=0, device=cpu) = aten::mul(%6799, %6802), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:292:0
  %6818 : Long(requires_grad=0, device=cpu) = aten::mul(%6817, %6805), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:292:0
  %6819 : Long(requires_grad=0, device=cpu) = aten::mul(%6818, %6808), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:292:0
  %6820 : Long(requires_grad=0, device=cpu) = aten::mul(%6819, %6811), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:292:0
  %6822 : int[] = prim::ListConstruct(%6820, %6814), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6823 : Float(3096000, 128, strides=[128, 1], requires_grad=0, device=cuda:0) = aten::reshape(%x.1, %6822), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:295:0
  %6824 : Tensor?[] = prim::ListConstruct(%kept.1), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6825 : Float(1395622, 128, strides=[128, 1], requires_grad=0, device=cuda:0) = aten::index(%6823, %6824), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:297:0
  %6839 : Tensor?[] = prim::ListConstruct(%indices), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %x.3 : Float(1395622, 128, strides=[128, 1], requires_grad=0, device=cuda:0) = aten::index(%6825, %6839), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:300:0
  %6864 : Long(device=cpu) = aten::size(%x.3, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:146:0
  %6870 : int[] = prim::ListConstruct(%6864), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %8529 : Long(device=cpu) = prim::Constant[value={11}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6873 : Device = prim::Constant[value="cuda:0"](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:146:0
  %kept : Bool(1395622, strides=[1], requires_grad=0, device=cuda:0) = aten::ones(%6870, %8529, %6769, %6873, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:146:0
  %6880 : Long(1395621, strides=[1], requires_grad=0, device=cuda:0) = aten::slice(%ranks, %8515, %8516, %8524, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:147:0
  %8530 : Long(device=cpu) = prim::Constant[value={-1}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6885 : Long(1395621, strides=[1], requires_grad=0, device=cuda:0) = aten::slice(%ranks, %8515, %8515, %8530, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:147:0
  %6886 : Bool(1395621, strides=[1], requires_grad=0, device=cuda:0) = aten::ne(%6880, %6885), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:147:0
  %6903 : Bool(1, strides=[1], requires_grad=0, device=cuda:0) = aten::slice(%kept, %8515, %8515, %8516, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:56:0
  %6908 : Int(1, strides=[1], requires_grad=0, device=cuda:0) = aten::to(%6903, %8518, %8520, %8520, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:22:0
  %6913 : Int(1395621, strides=[1], requires_grad=0, device=cuda:0) = aten::to(%6886, %8518, %8520, %8520, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:22:0
  %6914 : Tensor[] = prim::ListConstruct(%6908, %6913), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %6916 : Int(1395622, strides=[1], requires_grad=0, device=cuda:0) = aten::cat(%6914, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:23:0
  %out.11 : Bool(1395622, strides=[1], requires_grad=0, device=cuda:0) = aten::to(%6916, %8529, %8520, %8520, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:23:0
  %8488 : Tensor?[] = prim::ListConstruct()
  %out.1 : Bool(1395622, strides=[1], requires_grad=0, device=cuda:0) = aten::expand_as(%out.11, %kept), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:23:0
  %8491 : Tensor = onnx::Placeholder[name="index_put_"](%kept), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:23:0
    block0():
      %8492 : Bool(1395622, strides=[1], requires_grad=0, device=cuda:0) = aten::index_put_(%kept, %8488, %out.1, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:23:0
      -> (%8492)
  %8513 : Long(28905, strides=[1], requires_grad=0, device=cuda:0) = aten::where[_outputs=1](%8491), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:148:0
  %interval_starts : Int(28905, strides=[1], requires_grad=0, device=cuda:0) = aten::to(%8513, %8518, %8520, %8520, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:148:0
  %interval_lengths.1 : Int(28905, strides=[1], requires_grad=0, device=cuda:0) = aten::zeros_like(%interval_starts, %6769, %6769, %6769, %8520, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:149:0
  %6941 : Int(28904, strides=[1], requires_grad=0, device=cuda:0) = aten::slice(%interval_starts, %8515, %8516, %8524, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:150:0
  %6946 : Int(28904, strides=[1], requires_grad=0, device=cuda:0) = aten::slice(%interval_starts, %8515, %8515, %8530, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:150:0
  %value.1 : Int(28904, strides=[1], requires_grad=0, device=cuda:0) = aten::sub(%6941, %6946, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:150:0
  %self_end : Int(1, strides=[1], requires_grad=0, device=cuda:0) = aten::slice(%interval_lengths.1, %8515, %8530, %8524, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:66:0
  %6967 : Tensor[] = prim::ListConstruct(%value.1, %self_end), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %out.13 : Int(28905, strides=[1], requires_grad=0, device=cuda:0) = aten::cat(%6967, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:24:0
  %8493 : Tensor?[] = prim::ListConstruct()
  %out : Int(28905, strides=[1], requires_grad=0, device=cuda:0) = aten::expand_as(%out.13, %interval_lengths.1), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:23:0
  %8496 : Tensor = onnx::Placeholder[name="index_put_"](%interval_lengths.1), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:23:0
    block0():
      %interval_lengths : Int(28905, strides=[1], requires_grad=0, device=cuda:0) = aten::index_put_(%interval_lengths.1, %8493, %out, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:23:0
      -> (%interval_lengths)
  %6980 : Int(requires_grad=0, device=cuda:0) = aten::select(%interval_starts, %8515, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:151:0
  %6982 : Long(requires_grad=0, device=cuda:0) = aten::sub(%6864, %6980, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:151:0
  %6985 : Int(requires_grad=0, device=cuda:0) = aten::select(%8496, %8515, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:23:0
  %8498 : Tensor?[] = prim::ListConstruct()
  %8499 : Long(requires_grad=0, device=cuda:0) = aten::expand_as(%6982, %6985), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:23:0
  %8501 : Tensor = onnx::Placeholder[name="index_put_"](%8496), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:23:0
    block0():
      %8502 : Int(requires_grad=0, device=cuda:0) = aten::select(%8496, %8515, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:23:0
      %8503 : Int(requires_grad=0, device=cuda:0) = aten::index_put_(%8502, %8498, %8499, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:23:0
      -> (%8503)
  %coords : Int(1395622, 4, strides=[4, 1], requires_grad=0, device=cuda:0) = aten::to(%geom_feats, %8518, %8520, %8520, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:154:0
  %6993 : Float(1, 1, 360, 360, 128, strides=[16588800, 16588800, 46080, 128, 1], requires_grad=0, device=cuda:0) = ^QuickCumsumCuda[inplace=0, module="projects.BEVFusion.bevfusion.ops.bev_pool.bev_pool", Subgraph=<Graph>](1, 1, 360, 360)(%x.3, %coords, %8501, %interval_starts), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/torch/autograd/function.py:576:0
  %8325 : int[] = prim::Constant[value=[0, 4, 1, 2, 3]]()
  %7011 : Float(1, 128, 1, 360, 360, strides=[16588800, 1, 16588800, 46080, 128], requires_grad=0, device=cuda:0) = aten::permute(%6993, %8325), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:160:0
  %7013 : Float(1, 128, 1, 360, 360, strides=[16588800, 129600, 129600, 360, 1], requires_grad=0, device=cuda:0) = aten::contiguous(%7011, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/ops/bev_pool/bev_pool.py:160:0
  %8514 : Float(1, 128, 360, 360, strides=[16588800, 129600, 360, 1], requires_grad=0, device=cuda:0) = aten::unbind[_outputs=1](%7013, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /workspace/projects/BEVFusion/bevfusion/depth_lss.py:304:0
  %7017 : Tensor[] = prim::ListConstruct(%8514), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform
  %input.15 : Float(1, 128, 360, 360, strides=[16588800, 129600, 360, 1], requires_grad=0, device=cuda:0) = aten::cat(%7017, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:24:0
  %input.17 : Float(1, 128, 360, 360, strides=[16588800, 129600, 360, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.15, %mod.view_transform.downsample.net.0.weight, %6769, %8312, %8312, %8312, %8520, %8315, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DownSampleNet::downsample/torch.nn.modules.container.Sequential::net/torch.nn.modules.conv.Conv2d::net.0 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:543:0
  %input.19 : Float(1, 128, 360, 360, strides=[16588800, 129600, 360, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.17, %mod.view_transform.downsample.net.1.weight, %mod.view_transform.downsample.net.1.bias, %mod.view_transform.downsample.net.1.running_mean, %mod.view_transform.downsample.net.1.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DownSampleNet::downsample/torch.nn.modules.container.Sequential::net/torch.nn.modules.batchnorm.BatchNorm2d::net.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8475 : Float(1, 128, 360, 360, strides=[16588800, 129600, 360, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.19), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DownSampleNet::downsample/torch.nn.modules.container.Sequential::net/torch.nn.modules.activation.ReLU::net.2 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %8330 : int[] = prim::Constant[value=[2, 2]]()
  %input.23 : Float(1, 128, 180, 180, strides=[4147200, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8475, %mod.view_transform.downsample.net.3.weight, %6769, %8330, %8312, %8312, %8520, %8315, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DownSampleNet::downsample/torch.nn.modules.container.Sequential::net/torch.nn.modules.conv.Conv2d::net.3 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:543:0
  %input.25 : Float(1, 128, 180, 180, strides=[4147200, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.23, %mod.view_transform.downsample.net.4.weight, %mod.view_transform.downsample.net.4.bias, %mod.view_transform.downsample.net.4.running_mean, %mod.view_transform.downsample.net.4.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DownSampleNet::downsample/torch.nn.modules.container.Sequential::net/torch.nn.modules.batchnorm.BatchNorm2d::net.4 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8476 : Float(1, 128, 180, 180, strides=[4147200, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.25), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DownSampleNet::downsample/torch.nn.modules.container.Sequential::net/torch.nn.modules.activation.ReLU::net.5 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %input.29 : Float(1, 128, 180, 180, strides=[4147200, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8476, %mod.view_transform.downsample.net.6.weight, %6769, %8312, %8312, %8312, %8520, %8315, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DownSampleNet::downsample/torch.nn.modules.container.Sequential::net/torch.nn.modules.conv.Conv2d::net.6 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:543:0
  %input.31 : Float(1, 128, 180, 180, strides=[4147200, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.29, %mod.view_transform.downsample.net.7.weight, %mod.view_transform.downsample.net.7.bias, %mod.view_transform.downsample.net.7.running_mean, %mod.view_transform.downsample.net.7.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DownSampleNet::downsample/torch.nn.modules.container.Sequential::net/torch.nn.modules.batchnorm.BatchNorm2d::net.7 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8477 : Float(1, 128, 180, 180, strides=[4147200, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.31), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.depth_lss.NonLinearLSSTransform::view_transform/projects.BEVFusion.bevfusion.depth_lss.DownSampleNet::downsample/torch.nn.modules.container.Sequential::net/torch.nn.modules.activation.ReLU::net.8 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %7099 : Long(device=cpu) = aten::size(%8477, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:244:0
  %7132 : Float(1, 128, 180, 180, strides=[4147200, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8477, %mod.bbox_head.shared_conv.weight, %mod.bbox_head.shared_conv.bias, %8312, %8312, %8312, %8520, %8315, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/torch.nn.modules.conv.Conv2d::shared_conv # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:543:0
  %7137 : Long(device=cpu) = aten::size(%7132, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:250:0
  %7147 : int[] = prim::ListConstruct(%7099, %7137, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %7148 : Float(1, 128, 32400, strides=[4147200, 32400, 1], requires_grad=0, device=cuda:0) = aten::view(%7132, %7147), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:250:0
  %7149 : Float(1, 32400, 2, strides=[64800, 1, 32400], requires_grad=0, device=cpu) = prim::Constant[value=<Tensor>](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/repeat.py:27:0
  %7152 : int[] = prim::ListConstruct(%7099, %8516, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %7153 : Float(1, 32400, 2, strides=[64800, 2, 1], requires_grad=0, device=cpu) = aten::repeat(%7149, %7152), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/repeat.py:27:0
  %8531 : Long(device=cpu) = prim::Constant[value={6}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %7161 : Float(1, 32400, 2, strides=[64800, 2, 1], requires_grad=0, device=cuda:0) = aten::to(%7153, %8531, %8515, %6873, %6769, %8520, %8520, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:251:0
  %input.35 : Float(1, 128, 180, 180, strides=[4147200, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::to(%7132, %8531, %8520, %8520, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:258:0
  %input.37 : Float(1, 128, 180, 180, strides=[4147200, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.35, %mod.bbox_head.heatmap_head.0.conv.weight, %6769, %8312, %8312, %8312, %8520, %8315, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/torch.nn.modules.container.Sequential::heatmap_head/mmcv.cnn.bricks.conv_module.ConvModule::heatmap_head.0/torch.nn.modules.conv.Conv2d::conv # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:543:0
  %input.39 : Float(1, 128, 180, 180, strides=[4147200, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.37, %mod.bbox_head.heatmap_head.0.bn.weight, %mod.bbox_head.heatmap_head.0.bn.bias, %mod.bbox_head.heatmap_head.0.bn.running_mean, %mod.bbox_head.heatmap_head.0.bn.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/torch.nn.modules.container.Sequential::heatmap_head/mmcv.cnn.bricks.conv_module.ConvModule::heatmap_head.0/torch.nn.modules.batchnorm.BatchNorm2d::bn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8478 : Float(1, 128, 180, 180, strides=[4147200, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.39), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/torch.nn.modules.container.Sequential::heatmap_head/mmcv.cnn.bricks.conv_module.ConvModule::heatmap_head.0/torch.nn.modules.activation.ReLU::activate # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %7211 : Float(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8478, %mod.bbox_head.heatmap_head.1.weight, %mod.bbox_head.heatmap_head.1.bias, %8312, %8312, %8312, %8520, %8315, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/torch.nn.modules.container.Sequential::heatmap_head/torch.nn.modules.conv.Conv2d::heatmap_head.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:543:0
  %7212 : Float(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::detach(%7211), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:259:0
  %7213 : Float(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::sigmoid(%7212), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:259:0
  %local_max : Float(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::zeros_like(%7213, %6769, %6769, %6769, %8520, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:260:0
  %7224 : Float(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::slice(%7213, %8515, %8515, %8524, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:264:0
  %7236 : Float(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::slice(%7224, %8517, %8515, %8524, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:264:0
  %7241 : Float(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::slice(%7236, %8518, %8515, %8524, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:264:0
  %8441 : Long(4, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value= 0  1  2  3 [ CUDALongType{4} ]]()
  %8442 : Tensor[] = prim::ListConstruct(%6769, %8441)
  %7244 : Float(1, 4, 180, 180, strides=[129600, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::index(%7241, %8442), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:264:0
  %8352 : int[] = prim::Constant[value=[3, 3]]()
  %local_max_inner : Float(1, 4, 178, 178, strides=[126736, 31684, 178, 1], requires_grad=0, device=cuda:0) = aten::max_pool2d(%7244, %8352, %8312, %8315, %8312, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:827:0
  %8357 : int[] = prim::Constant[value=[4, 178, 178]]()
  %7285 : Float(4, 178, 178, strides=[31684, 178, 1], requires_grad=0, device=cuda:0) = aten::view(%local_max_inner, %8357), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:27:0
  %8504 : Tensor = onnx::Placeholder[name="index_put_"](%local_max), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:27:0
    block0():
      %8505 : Float(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::slice(%local_max, %8515, %8515, %8524, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:27:0
      %8506 : Float(1, 5, 178, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::slice(%8505, %8517, %8516, %8530, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:27:0
      %8507 : Float(1, 5, 178, 178, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::slice(%8506, %8518, %8516, %8530, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:27:0
      %8508 : Float(1, 5, 178, 178, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::index_put_(%8507, %8442, %7285, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:27:0
      -> (%8508)
  %8447 : Long(1, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value={4}]()
  %8448 : Tensor[] = prim::ListConstruct(%6769, %8447)
  %7304 : Float(1, 1, 180, 180, strides=[32400, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::index(%7224, %8448), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:279:0
  %8362 : int[] = prim::Constant[value=[180, 180]]()
  %7320 : Float(180, 180, strides=[180, 1], requires_grad=0, device=cuda:0) = aten::view(%7304, %8362), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:27:0
  %8509 : Tensor = onnx::Placeholder[name="index_put_"](%8504), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:27:0
    block0():
      %8510 : Float(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::slice(%local_max, %8515, %8515, %8524, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:27:0
      %8511 : Float(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::index_put_(%8510, %8448, %7320, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/tensor_setitem.py:27:0
      -> (%8511)
  %7325 : Bool(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::eq(%7213, %8509), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:285:0
  %7326 : Float(1, 5, 180, 180, strides=[162000, 32400, 180, 1], requires_grad=0, device=cuda:0) = aten::mul(%7213, %7325), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:285:0
  %7331 : Long(device=cpu) = aten::size(%7326, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:286:0
  %7341 : int[] = prim::ListConstruct(%7099, %7331, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %7342 : Float(1, 5, 32400, strides=[162000, 32400, 1], requires_grad=0, device=cuda:0) = aten::view(%7326, %7341), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:286:0
  %7344 : int[] = prim::ListConstruct(%7099, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %7345 : Float(1, 162000, strides=[162000, 1], requires_grad=0, device=cuda:0) = aten::view(%7342, %7344), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:289:0
  %7348 : Long(1, 162000, strides=[162000, 1], requires_grad=0, device=cuda:0) = aten::argsort(%7345, %8530, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:289:0
  %8532 : Long(device=cpu) = prim::Constant[value={500}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %top_proposals : Long(1, 500, strides=[162000, 1], requires_grad=0, device=cuda:0) = aten::slice(%7348, %8516, %8515, %8532, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:289:0
  %7361 : Long(device=cpu) = aten::size(%7342, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:290:0
  %7363 : Long(1, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::floor_divide(%top_proposals, %7361), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/torch/_tensor.py:1138:0
  %7374 : Long(1, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::mul(%7363, %7361), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/mod.py:20:0
  %7376 : Long(1, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::sub(%top_proposals, %7374, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/mod.py:20:0
  %7381 : Long(1, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::slice(%7376, %8515, %8515, %8524, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:293:0
  %7383 : Long(1, 1, 500, strides=[500, 500, 1], requires_grad=0, device=cuda:0) = aten::unsqueeze(%7381, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:293:0
  %7388 : Long(1, 1, 500, strides=[500, 500, 1], requires_grad=0, device=cuda:0) = aten::slice(%7383, %8517, %8515, %8524, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:293:0
  %7393 : Long(device=cpu) = aten::size(%7148, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:293:0
  %7401 : int[] = prim::ListConstruct(%8530, %7393, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %7403 : Long(1, 128, 500, strides=[500, 0, 1], requires_grad=0, device=cuda:0) = aten::expand(%7388, %7401, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:293:0
  %7406 : Float(1, 128, 500, strides=[64000, 500, 1], requires_grad=0, device=cuda:0) = aten::gather(%7148, %8530, %7403, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:292:0
  %7408 : Long(1, 500, 5, strides=[2500, 5, 1], requires_grad=0, device=cuda:0) = aten::one_hot(%7363, %8528), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:299:0
  %8364 : int[] = prim::Constant[value=[0, 2, 1]]()
  %7413 : Long(1, 5, 500, strides=[2500, 1, 5], requires_grad=0, device=cuda:0) = aten::permute(%7408, %8364), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:299:0
  %input.43 : Float(1, 5, 500, strides=[2500, 1, 5], requires_grad=0, device=cuda:0) = aten::to(%7413, %8531, %8520, %8520, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:300:0
  %8365 : int[] = prim::Constant[value=[1]]()
  %8366 : int[] = prim::Constant[value=[0]]()
  %7433 : Float(1, 128, 500, strides=[64000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.43, %mod.bbox_head.class_encoding.weight, %mod.bbox_head.class_encoding.bias, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/torch.nn.modules.conv.Conv1d::class_encoding # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %8472 : Tensor = aten::type_as(%7433, %7406)
  %8512 : Float(1, 128, 500, strides=[64000, 500, 1], requires_grad=0, device=cuda:0) = aten::add(%7406, %8472, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:301:0
  %7452 : Long(1, 500, 1, strides=[500, 1, 500], requires_grad=0, device=cuda:0) = aten::permute(%7388, %8364), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:304:0
  %7460 : Long(device=cpu) = aten::size(%7161, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:304:0
  %7465 : int[] = prim::ListConstruct(%8530, %8530, %7460), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %7467 : Long(1, 500, 2, strides=[500, 1, 0], requires_grad=0, device=cuda:0) = aten::expand(%7452, %7465, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:304:0
  %7470 : Float(1, 500, 2, strides=[1000, 2, 1], requires_grad=0, device=cuda:0) = aten::gather(%7161, %8516, %7467, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:303:0
  %7473 : Float(1, 2, 500, strides=[1000, 1, 2], requires_grad=0, device=cuda:0) = aten::transpose(%7470, %8516, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::self_posembed # /workspace/projects/BEVFusion/bevfusion/transformer.py:20:0
  %input.45 : Float(1, 2, 500, strides=[1000, 500, 1], requires_grad=0, device=cuda:0) = aten::contiguous(%7473, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::self_posembed # /workspace/projects/BEVFusion/bevfusion/transformer.py:20:0
  %input.47 : Float(1, 128, 500, strides=[64000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.45, %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.0.weight, %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.0.bias, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::self_posembed/torch.nn.modules.container.Sequential::position_embedding_head/torch.nn.modules.conv.Conv1d::position_embedding_head.0 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.49 : Float(1, 128, 500, strides=[64000, 500, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.47, %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.1.weight, %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.1.bias, %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.1.running_mean, %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.1.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::self_posembed/torch.nn.modules.container.Sequential::position_embedding_head/torch.nn.modules.batchnorm.BatchNorm1d::position_embedding_head.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8479 : Float(1, 128, 500, strides=[64000, 500, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.49), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::self_posembed/torch.nn.modules.container.Sequential::position_embedding_head/torch.nn.modules.activation.ReLU::position_embedding_head.2 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %7511 : Float(1, 128, 500, strides=[64000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8479, %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.3.weight, %mod.bbox_head.decoder.0.self_posembed.position_embedding_head.3.bias, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::self_posembed/torch.nn.modules.container.Sequential::position_embedding_head/torch.nn.modules.conv.Conv1d::position_embedding_head.3 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %7514 : Float(1, 500, 128, strides=[64000, 1, 500], requires_grad=0, device=cuda:0) = aten::transpose(%7511, %8516, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0 # /workspace/projects/BEVFusion/bevfusion/transformer.py:75:0
  %7517 : Float(1, 2, 32400, strides=[64800, 1, 2], requires_grad=0, device=cuda:0) = aten::transpose(%7161, %8516, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::cross_posembed # /workspace/projects/BEVFusion/bevfusion/transformer.py:20:0
  %input.53 : Float(1, 2, 32400, strides=[64800, 32400, 1], requires_grad=0, device=cuda:0) = aten::contiguous(%7517, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::cross_posembed # /workspace/projects/BEVFusion/bevfusion/transformer.py:20:0
  %input.55 : Float(1, 128, 32400, strides=[4147200, 32400, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.53, %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.0.weight, %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.0.bias, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::cross_posembed/torch.nn.modules.container.Sequential::position_embedding_head/torch.nn.modules.conv.Conv1d::position_embedding_head.0 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.57 : Float(1, 128, 32400, strides=[4147200, 32400, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.55, %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.1.weight, %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.1.bias, %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.1.running_mean, %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.1.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::cross_posembed/torch.nn.modules.container.Sequential::position_embedding_head/torch.nn.modules.batchnorm.BatchNorm1d::position_embedding_head.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8480 : Float(1, 128, 32400, strides=[4147200, 32400, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.57), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::cross_posembed/torch.nn.modules.container.Sequential::position_embedding_head/torch.nn.modules.activation.ReLU::position_embedding_head.2 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %7555 : Float(1, 128, 32400, strides=[4147200, 32400, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8480, %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.3.weight, %mod.bbox_head.decoder.0.cross_posembed.position_embedding_head.3.bias, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/projects.BEVFusion.bevfusion.transformer.PositionEncodingLearned::cross_posembed/torch.nn.modules.container.Sequential::position_embedding_head/torch.nn.modules.conv.Conv1d::position_embedding_head.3 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %7558 : Float(1, 32400, 128, strides=[4147200, 1, 32400], requires_grad=0, device=cuda:0) = aten::transpose(%7555, %8516, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0 # /workspace/projects/BEVFusion/bevfusion/transformer.py:79:0
  %7561 : Float(1, 500, 128, strides=[64000, 1, 500], requires_grad=0, device=cuda:0) = aten::transpose(%8512, %8516, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0 # /workspace/projects/BEVFusion/bevfusion/transformer.py:82:0
  %7564 : Float(1, 32400, 128, strides=[4147200, 1, 32400], requires_grad=0, device=cuda:0) = aten::transpose(%7148, %8516, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0 # /workspace/projects/BEVFusion/bevfusion/transformer.py:83:0
  %7566 : Float(1, 500, 128, strides=[64000, 1, 500], requires_grad=0, device=cuda:0) = aten::add(%7561, %7514, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0 # /workspace/projects/BEVFusion/bevfusion/transformer.py:89:0
  %7568 : Float(1, 500, 128, strides=[64000, 1, 500], requires_grad=0, device=cuda:0) = aten::add(%7561, %7514, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:527:0
  %7570 : Float(1, 500, 128, strides=[64000, 1, 500], requires_grad=0, device=cuda:0) = aten::add(%7561, %7514, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:529:0
  %query.1 : Float(500, 1, 128, strides=[1, 64000, 500], requires_grad=0, device=cuda:0) = aten::transpose(%7568, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:538:0
  %key.1 : Float(500, 1, 128, strides=[1, 64000, 500], requires_grad=0, device=cuda:0) = aten::transpose(%7570, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:539:0
  %value.3 : Float(500, 1, 128, strides=[1, 64000, 500], requires_grad=0, device=cuda:0) = aten::transpose(%7566, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:540:0
  %7581 : Long(device=cpu) = aten::size(%query.1, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6237:0
  %7587 : Long(device=cpu) = aten::size(%query.1, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6237:0
  %7592 : Long(device=cpu) = aten::size(%query.1, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6237:0
  %7606 : Long(requires_grad=0, device=cpu) = prim::Constant[value={8}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6281:0
  %7607 : str = prim::Constant[value="trunc"](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6281:0
  %7608 : Long(requires_grad=0, device=cpu) = aten::div(%7592, %7606, %7607), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6281:0
  %8452 : Float(128, 128, strides=[128, 1], requires_grad=1, device=cuda:0), %8453 : Float(128, 128, strides=[128, 1], requires_grad=1, device=cuda:0), %8454 : Float(128, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = prim::ConstantChunk[chunks=3, dim=0](%mod.bbox_head.decoder.0.self_attn.attn.in_proj_weight), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:5724:0
  %8455 : Float(128, strides=[1], requires_grad=1, device=cuda:0), %8456 : Float(128, strides=[1], requires_grad=1, device=cuda:0), %8457 : Float(128, strides=[1], requires_grad=1, device=cuda:0) = prim::ConstantChunk[chunks=3, dim=0](%mod.bbox_head.decoder.0.self_attn.attn.in_proj_bias), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:5728:0
  %7651 : Float(500, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::linear(%query.1, %8452, %8455), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:5729:0
  %7652 : Float(500, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::linear(%key.1, %8453, %8456), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:5729:0
  %7653 : Float(500, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::linear(%value.3, %8454, %8457), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:5729:0
  %8533 : Long(device=cpu) = prim::Constant[value={8}]()
  %8459 : Long(requires_grad=0, device=cpu) = aten::mul(%7587, %8533), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6370:0
  %7657 : int[] = prim::ListConstruct(%7581, %8459, %7608), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn
  %7658 : Float(500, 8, 16, strides=[128, 16, 1], requires_grad=0, device=cuda:0) = aten::view(%7651, %7657), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6370:0
  %7661 : Float(8, 500, 16, strides=[16, 128, 1], requires_grad=0, device=cuda:0) = aten::transpose(%7658, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6370:0
  %7663 : Long(device=cpu) = aten::size(%7652, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6372:0
  %7675 : int[] = prim::ListConstruct(%7663, %8459, %7608), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn
  %7676 : Float(500, 8, 16, strides=[128, 16, 1], requires_grad=0, device=cuda:0) = aten::view(%7652, %7675), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6372:0
  %7679 : Float(8, 500, 16, strides=[16, 128, 1], requires_grad=0, device=cuda:0) = aten::transpose(%7676, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6372:0
  %7681 : Long(device=cpu) = aten::size(%7653, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6383:0
  %7693 : int[] = prim::ListConstruct(%7681, %8459, %7608), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn
  %7694 : Float(500, 8, 16, strides=[128, 16, 1], requires_grad=0, device=cuda:0) = aten::view(%7653, %7693), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6383:0
  %7697 : Float(8, 500, 16, strides=[16, 128, 1], requires_grad=0, device=cuda:0) = aten::transpose(%7694, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6383:0
  %8534 : Double(device=cpu) = prim::Constant[value={0.25}]()
  %8461 : Float(8, 500, 16, strides=[16, 128, 1], requires_grad=0, device=cuda:0) = aten::mul(%7661, %8534), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6436:0
  %8535 : Long(device=cpu) = prim::Constant[value={-2}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn
  %7715 : Float(8, 16, 500, strides=[16, 1, 128], requires_grad=0, device=cuda:0) = aten::transpose(%7679, %8535, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6447:0
  %attn_output_weights.1 : Float(8, 500, 500, strides=[250000, 500, 1], requires_grad=0, device=cuda:0) = aten::bmm(%8461, %7715), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6447:0
  %7719 : Float(8, 500, 500, strides=[250000, 500, 1], requires_grad=0, device=cuda:0) = aten::softmax(%attn_output_weights.1, %8530, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2137:0
  %7720 : Float(8, 500, 16, strides=[8000, 16, 1], requires_grad=0, device=cuda:0) = aten::bmm(%7719, %7697), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6452:0
  %7723 : Float(500, 8, 16, strides=[16, 8000, 1], requires_grad=0, device=cuda:0) = aten::transpose(%7720, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6455:0
  %7725 : Float(500, 8, 16, strides=[128, 16, 1], requires_grad=0, device=cuda:0) = aten::contiguous(%7723, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6455:0
  %7726 : Long(requires_grad=0, device=cpu) = aten::mul(%7581, %7587), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6455:0
  %7728 : int[] = prim::ListConstruct(%7726, %7592), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn
  %7729 : Float(500, 128, strides=[128, 1], requires_grad=0, device=cuda:0) = aten::view(%7725, %7728), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6455:0
  %7730 : Float(500, 128, strides=[128, 1], requires_grad=0, device=cuda:0) = aten::linear(%7729, %mod.bbox_head.decoder.0.self_attn.attn.out_proj.weight, %mod.bbox_head.decoder.0.self_attn.attn.out_proj.bias), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6457:0
  %7732 : Long(device=cpu) = aten::size(%7730, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6458:0
  %7735 : int[] = prim::ListConstruct(%7581, %7587, %7732), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn
  %7736 : Float(500, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::view(%7730, %7735), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6458:0
  %input.61 : Float(1, 500, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::transpose(%7736, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:550:0
  %8536 : Double(device=cpu) = prim::Constant[value={0}](), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.dropout.Dropout::proj_drop
  %input.63 : Float(1, 500, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::dropout(%input.61, %8536, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/torch.nn.modules.dropout.Dropout::proj_drop # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1422:0
  %7753 : Float(1, 500, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::dropout(%input.63, %8522, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn/mmcv.cnn.bricks.drop.Dropout::dropout_layer # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1422:0
  %input.65 : Float(1, 500, 128, strides=[64000, 1, 500], requires_grad=0, device=cuda:0) = aten::add(%7561, %7753, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::self_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:552:0
  %8386 : int[] = prim::Constant[value=[128]]()
  %7760 : Float(1, 500, 128, strides=[64000, 128, 1], requires_grad=0, device=cuda:0) = aten::layer_norm(%input.65, %8386, %mod.bbox_head.decoder.0.norms.0.weight, %mod.bbox_head.decoder.0.norms.0.bias, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/torch.nn.modules.normalization.LayerNorm::norms.0 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2905:0
  %7762 : Float(1, 32400, 128, strides=[4147200, 1, 32400], requires_grad=0, device=cuda:0) = aten::add(%7564, %7558, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0 # /workspace/projects/BEVFusion/bevfusion/transformer.py:101:0
  %7764 : Float(1, 500, 128, strides=[64000, 128, 1], requires_grad=0, device=cuda:0) = aten::add(%7760, %7514, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:527:0
  %query : Float(500, 1, 128, strides=[128, 64000, 1], requires_grad=0, device=cuda:0) = aten::transpose(%7764, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:538:0
  %key : Float(32400, 1, 128, strides=[1, 4147200, 32400], requires_grad=0, device=cuda:0) = aten::transpose(%7762, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:539:0
  %7777 : Long(device=cpu) = aten::size(%query, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6237:0
  %7783 : Long(device=cpu) = aten::size(%query, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6237:0
  %7788 : Long(device=cpu) = aten::size(%query, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6237:0
  %7804 : Long(requires_grad=0, device=cpu) = aten::div(%7788, %7606, %7607), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6281:0
  %8462 : Float(128, 128, strides=[128, 1], requires_grad=1, device=cuda:0), %8463 : Float(128, 128, strides=[128, 1], requires_grad=1, device=cuda:0), %8464 : Float(128, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = prim::ConstantChunk[chunks=3, dim=0](%mod.bbox_head.decoder.0.cross_attn.attn.in_proj_weight), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:5724:0
  %8465 : Float(128, strides=[1], requires_grad=1, device=cuda:0), %8466 : Float(128, strides=[1], requires_grad=1, device=cuda:0), %8467 : Float(128, strides=[1], requires_grad=1, device=cuda:0) = prim::ConstantChunk[chunks=3, dim=0](%mod.bbox_head.decoder.0.cross_attn.attn.in_proj_bias), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:5728:0
  %7847 : Float(500, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::linear(%query, %8462, %8465), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:5729:0
  %7848 : Float(32400, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::linear(%key, %8463, %8466), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:5729:0
  %7849 : Float(32400, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::linear(%key, %8464, %8467), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:5729:0
  %8537 : Long(device=cpu) = prim::Constant[value={8}]()
  %8469 : Long(requires_grad=0, device=cpu) = aten::mul(%7783, %8537), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6370:0
  %7853 : int[] = prim::ListConstruct(%7777, %8469, %7804), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn
  %7854 : Float(500, 8, 16, strides=[128, 16, 1], requires_grad=0, device=cuda:0) = aten::view(%7847, %7853), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6370:0
  %7857 : Float(8, 500, 16, strides=[16, 128, 1], requires_grad=0, device=cuda:0) = aten::transpose(%7854, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6370:0
  %7859 : Long(device=cpu) = aten::size(%7848, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6372:0
  %7871 : int[] = prim::ListConstruct(%7859, %8469, %7804), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn
  %7872 : Float(32400, 8, 16, strides=[128, 16, 1], requires_grad=0, device=cuda:0) = aten::view(%7848, %7871), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6372:0
  %7875 : Float(8, 32400, 16, strides=[16, 128, 1], requires_grad=0, device=cuda:0) = aten::transpose(%7872, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6372:0
  %7877 : Long(device=cpu) = aten::size(%7849, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6383:0
  %7889 : int[] = prim::ListConstruct(%7877, %8469, %7804), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn
  %7890 : Float(32400, 8, 16, strides=[128, 16, 1], requires_grad=0, device=cuda:0) = aten::view(%7849, %7889), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6383:0
  %7893 : Float(8, 32400, 16, strides=[16, 128, 1], requires_grad=0, device=cuda:0) = aten::transpose(%7890, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6383:0
  %8538 : Double(device=cpu) = prim::Constant[value={0.25}]()
  %8471 : Float(8, 500, 16, strides=[16, 128, 1], requires_grad=0, device=cuda:0) = aten::mul(%7857, %8538), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6436:0
  %7911 : Float(8, 16, 32400, strides=[16, 1, 128], requires_grad=0, device=cuda:0) = aten::transpose(%7875, %8535, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6447:0
  %attn_output_weights : Float(8, 500, 32400, strides=[16200000, 32400, 1], requires_grad=0, device=cuda:0) = aten::bmm(%8471, %7911), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6447:0
  %7915 : Float(8, 500, 32400, strides=[16200000, 32400, 1], requires_grad=0, device=cuda:0) = aten::softmax(%attn_output_weights, %8530, %6769), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2137:0
  %7916 : Float(8, 500, 16, strides=[8000, 16, 1], requires_grad=0, device=cuda:0) = aten::bmm(%7915, %7893), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6452:0
  %7919 : Float(500, 8, 16, strides=[16, 8000, 1], requires_grad=0, device=cuda:0) = aten::transpose(%7916, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6455:0
  %7921 : Float(500, 8, 16, strides=[128, 16, 1], requires_grad=0, device=cuda:0) = aten::contiguous(%7919, %8515), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6455:0
  %7922 : Long(requires_grad=0, device=cpu) = aten::mul(%7777, %7783), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6455:0
  %7924 : int[] = prim::ListConstruct(%7922, %7788), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn
  %7925 : Float(500, 128, strides=[128, 1], requires_grad=0, device=cuda:0) = aten::view(%7921, %7924), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6455:0
  %7926 : Float(500, 128, strides=[128, 1], requires_grad=0, device=cuda:0) = aten::linear(%7925, %mod.bbox_head.decoder.0.cross_attn.attn.out_proj.weight, %mod.bbox_head.decoder.0.cross_attn.attn.out_proj.bias), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6457:0
  %7928 : Long(device=cpu) = aten::size(%7926, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6458:0
  %7931 : int[] = prim::ListConstruct(%7777, %7783, %7928), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn
  %7932 : Float(500, 1, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::view(%7926, %7931), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.activation.MultiheadAttention::attn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:6458:0
  %input.67 : Float(1, 500, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::transpose(%7932, %8515, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:550:0
  %input.69 : Float(1, 500, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::dropout(%input.67, %8536, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/torch.nn.modules.dropout.Dropout::proj_drop # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1422:0
  %7949 : Float(1, 500, 128, strides=[128, 128, 1], requires_grad=0, device=cuda:0) = aten::dropout(%input.69, %8522, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn/mmcv.cnn.bricks.drop.Dropout::dropout_layer # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1422:0
  %input.71 : Float(1, 500, 128, strides=[64000, 128, 1], requires_grad=0, device=cuda:0) = aten::add(%7760, %7949, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.MultiheadAttention::cross_attn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:552:0
  %x.5 : Float(1, 500, 128, strides=[64000, 128, 1], requires_grad=0, device=cuda:0) = aten::layer_norm(%input.71, %8386, %mod.bbox_head.decoder.0.norms.1.weight, %mod.bbox_head.decoder.0.norms.1.bias, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/torch.nn.modules.normalization.LayerNorm::norms.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2905:0
  %input.73 : Float(1, 500, 256, strides=[128000, 256, 1], requires_grad=0, device=cuda:0) = aten::linear(%x.5, %mod.bbox_head.decoder.0.ffn.layers.0.0.weight, %mod.bbox_head.decoder.0.ffn.layers.0.0.bias), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.FFN::ffn/mmengine.model.base_module.Sequential::layers/mmengine.model.base_module.Sequential::layers.0/mmcv.cnn.bricks.wrappers.Linear::layers.0.0 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0
  %8481 : Float(1, 500, 256, strides=[128000, 256, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.73), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.FFN::ffn/mmengine.model.base_module.Sequential::layers/mmengine.model.base_module.Sequential::layers.0/torch.nn.modules.activation.ReLU::layers.0.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %x : Float(1, 500, 256, strides=[128000, 256, 1], requires_grad=0, device=cuda:0) = aten::dropout(%8481, %8522, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.FFN::ffn/mmengine.model.base_module.Sequential::layers/mmengine.model.base_module.Sequential::layers.0/torch.nn.modules.dropout.Dropout::layers.0.2 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1422:0
  %input.77 : Float(1, 500, 128, strides=[64000, 128, 1], requires_grad=0, device=cuda:0) = aten::linear(%x, %mod.bbox_head.decoder.0.ffn.layers.1.weight, %mod.bbox_head.decoder.0.ffn.layers.1.bias), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.FFN::ffn/mmengine.model.base_module.Sequential::layers/mmcv.cnn.bricks.wrappers.Linear::layers.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0
  %7965 : Float(1, 500, 128, strides=[64000, 128, 1], requires_grad=0, device=cuda:0) = aten::dropout(%input.77, %8522, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.FFN::ffn/mmengine.model.base_module.Sequential::layers/torch.nn.modules.dropout.Dropout::layers.2 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1422:0
  %input.79 : Float(1, 500, 128, strides=[64000, 128, 1], requires_grad=0, device=cuda:0) = aten::add(%x.5, %7965, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/mmcv.cnn.bricks.transformer.FFN::ffn # /opt/conda/lib/python3.11/site-packages/mmcv/cnn/bricks/transformer.py:635:0
  %7974 : Float(1, 500, 128, strides=[64000, 128, 1], requires_grad=0, device=cuda:0) = aten::layer_norm(%input.79, %8386, %mod.bbox_head.decoder.0.norms.2.weight, %mod.bbox_head.decoder.0.norms.2.bias, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0/torch.nn.modules.normalization.LayerNorm::norms.2 # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2905:0
  %input.81 : Float(1, 128, 500, strides=[64000, 1, 128], requires_grad=0, device=cuda:0) = aten::transpose(%7974, %8516, %8517), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/projects.BEVFusion.bevfusion.transformer.TransformerDecoderLayer::decoder.0 # /workspace/projects/BEVFusion/bevfusion/transformer.py:112:0
  %input.83 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.81, %mod.bbox_head.prediction_heads.0.center.0.conv.weight, %6769, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::center/mmcv.cnn.bricks.conv_module.ConvModule::center.0/torch.nn.modules.conv.Conv1d::conv # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.85 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.83, %mod.bbox_head.prediction_heads.0.center.0.bn.weight, %mod.bbox_head.prediction_heads.0.center.0.bn.bias, %mod.bbox_head.prediction_heads.0.center.0.bn.running_mean, %mod.bbox_head.prediction_heads.0.center.0.bn.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::center/mmcv.cnn.bricks.conv_module.ConvModule::center.0/torch.nn.modules.batchnorm.BatchNorm1d::bn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8482 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.85), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::center/mmcv.cnn.bricks.conv_module.ConvModule::center.0/torch.nn.modules.activation.ReLU::activate # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %8014 : Float(1, 2, 500, strides=[1000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8482, %mod.bbox_head.prediction_heads.0.center.1.weight, %mod.bbox_head.prediction_heads.0.center.1.bias, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::center/torch.nn.modules.conv.Conv1d::center.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.89 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.81, %mod.bbox_head.prediction_heads.0.height.0.conv.weight, %6769, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::height/mmcv.cnn.bricks.conv_module.ConvModule::height.0/torch.nn.modules.conv.Conv1d::conv # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.91 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.89, %mod.bbox_head.prediction_heads.0.height.0.bn.weight, %mod.bbox_head.prediction_heads.0.height.0.bn.bias, %mod.bbox_head.prediction_heads.0.height.0.bn.running_mean, %mod.bbox_head.prediction_heads.0.height.0.bn.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::height/mmcv.cnn.bricks.conv_module.ConvModule::height.0/torch.nn.modules.batchnorm.BatchNorm1d::bn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8483 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.91), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::height/mmcv.cnn.bricks.conv_module.ConvModule::height.0/torch.nn.modules.activation.ReLU::activate # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %8051 : Float(1, 1, 500, strides=[500, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8483, %mod.bbox_head.prediction_heads.0.height.1.weight, %mod.bbox_head.prediction_heads.0.height.1.bias, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::height/torch.nn.modules.conv.Conv1d::height.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.95 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.81, %mod.bbox_head.prediction_heads.0.dim.0.conv.weight, %6769, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::dim/mmcv.cnn.bricks.conv_module.ConvModule::dim.0/torch.nn.modules.conv.Conv1d::conv # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.97 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.95, %mod.bbox_head.prediction_heads.0.dim.0.bn.weight, %mod.bbox_head.prediction_heads.0.dim.0.bn.bias, %mod.bbox_head.prediction_heads.0.dim.0.bn.running_mean, %mod.bbox_head.prediction_heads.0.dim.0.bn.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::dim/mmcv.cnn.bricks.conv_module.ConvModule::dim.0/torch.nn.modules.batchnorm.BatchNorm1d::bn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8484 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.97), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::dim/mmcv.cnn.bricks.conv_module.ConvModule::dim.0/torch.nn.modules.activation.ReLU::activate # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %8088 : Float(1, 3, 500, strides=[1500, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8484, %mod.bbox_head.prediction_heads.0.dim.1.weight, %mod.bbox_head.prediction_heads.0.dim.1.bias, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::dim/torch.nn.modules.conv.Conv1d::dim.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.101 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.81, %mod.bbox_head.prediction_heads.0.rot.0.conv.weight, %6769, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::rot/mmcv.cnn.bricks.conv_module.ConvModule::rot.0/torch.nn.modules.conv.Conv1d::conv # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.103 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.101, %mod.bbox_head.prediction_heads.0.rot.0.bn.weight, %mod.bbox_head.prediction_heads.0.rot.0.bn.bias, %mod.bbox_head.prediction_heads.0.rot.0.bn.running_mean, %mod.bbox_head.prediction_heads.0.rot.0.bn.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::rot/mmcv.cnn.bricks.conv_module.ConvModule::rot.0/torch.nn.modules.batchnorm.BatchNorm1d::bn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8485 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.103), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::rot/mmcv.cnn.bricks.conv_module.ConvModule::rot.0/torch.nn.modules.activation.ReLU::activate # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %8125 : Float(1, 2, 500, strides=[1000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8485, %mod.bbox_head.prediction_heads.0.rot.1.weight, %mod.bbox_head.prediction_heads.0.rot.1.bias, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::rot/torch.nn.modules.conv.Conv1d::rot.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.107 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.81, %mod.bbox_head.prediction_heads.0.vel.0.conv.weight, %6769, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::vel/mmcv.cnn.bricks.conv_module.ConvModule::vel.0/torch.nn.modules.conv.Conv1d::conv # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.109 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.107, %mod.bbox_head.prediction_heads.0.vel.0.bn.weight, %mod.bbox_head.prediction_heads.0.vel.0.bn.bias, %mod.bbox_head.prediction_heads.0.vel.0.bn.running_mean, %mod.bbox_head.prediction_heads.0.vel.0.bn.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::vel/mmcv.cnn.bricks.conv_module.ConvModule::vel.0/torch.nn.modules.batchnorm.BatchNorm1d::bn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8486 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.109), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::vel/mmcv.cnn.bricks.conv_module.ConvModule::vel.0/torch.nn.modules.activation.ReLU::activate # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %8162 : Float(1, 2, 500, strides=[1000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8486, %mod.bbox_head.prediction_heads.0.vel.1.weight, %mod.bbox_head.prediction_heads.0.vel.1.bias, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::vel/torch.nn.modules.conv.Conv1d::vel.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.113 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.81, %mod.bbox_head.prediction_heads.0.heatmap.0.conv.weight, %6769, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::heatmap/mmcv.cnn.bricks.conv_module.ConvModule::heatmap.0/torch.nn.modules.conv.Conv1d::conv # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %input.115 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::batch_norm(%input.113, %mod.bbox_head.prediction_heads.0.heatmap.0.bn.weight, %mod.bbox_head.prediction_heads.0.heatmap.0.bn.bias, %mod.bbox_head.prediction_heads.0.heatmap.0.bn.running_mean, %mod.bbox_head.prediction_heads.0.heatmap.0.bn.running_var, %8520, %8522, %8523, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::heatmap/mmcv.cnn.bricks.conv_module.ConvModule::heatmap.0/torch.nn.modules.batchnorm.BatchNorm1d::bn # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2817:0
  %8487 : Float(1, 64, 500, strides=[32000, 500, 1], requires_grad=0, device=cuda:0) = aten::relu(%input.115), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::heatmap/mmcv.cnn.bricks.conv_module.ConvModule::heatmap.0/torch.nn.modules.activation.ReLU::activate # /opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:1699:0
  %8199 : Float(1, 5, 500, strides=[2500, 500, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%8487, %mod.bbox_head.prediction_heads.0.heatmap.1.weight, %mod.bbox_head.prediction_heads.0.heatmap.1.bias, %8365, %8366, %8365, %8520, %8366, %8516, %8520, %8520, %8521, %8521), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head/mmdet3d.models.dense_heads.centerpoint_head.SeparateHead::prediction_heads.0/torch.nn.modules.container.Sequential::heatmap/torch.nn.modules.conv.Conv1d::heatmap.1 # /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:366:0
  %8211 : Float(1, 2, 500, strides=[1000, 1, 2], requires_grad=0, device=cuda:0) = aten::permute(%7470, %8364), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:318:0
  %8213 : Float(1, 2, 500, strides=[1000, 500, 1], requires_grad=0, device=cuda:0) = aten::add(%8014, %8211, %8516), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:318:0
  %8438 : int[] = prim::Constant[value=[-1, 5, -1]]()
  %8239 : Long(1, 5, 500, strides=[500, 0, 1], requires_grad=0, device=cuda:0) = aten::expand(%7388, %8438, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:325:0
  %8242 : Float(1, 5, 500, strides=[2500, 500, 1], requires_grad=0, device=cuda:0) = aten::gather(%7342, %8530, %8239, %8520), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /workspace/projects/BEVFusion/bevfusion/bevfusion_head.py:324:0
  %8243 : Tensor[] = prim::ListConstruct(%8213), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %8245 : Float(1, 2, 500, strides=[1000, 500, 1], requires_grad=0, device=cuda:0) = aten::cat(%8243, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:24:0
  %8246 : Tensor[] = prim::ListConstruct(%8051), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %8248 : Float(1, 1, 500, strides=[500, 500, 1], requires_grad=0, device=cuda:0) = aten::cat(%8246, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:24:0
  %8249 : Tensor[] = prim::ListConstruct(%8088), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %8251 : Float(1, 3, 500, strides=[1500, 500, 1], requires_grad=0, device=cuda:0) = aten::cat(%8249, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:24:0
  %8252 : Tensor[] = prim::ListConstruct(%8125), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %8254 : Float(1, 2, 500, strides=[1000, 500, 1], requires_grad=0, device=cuda:0) = aten::cat(%8252, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:24:0
  %8255 : Tensor[] = prim::ListConstruct(%8162), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %8257 : Float(1, 2, 500, strides=[1000, 500, 1], requires_grad=0, device=cuda:0) = aten::cat(%8255, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:24:0
  %8258 : Tensor[] = prim::ListConstruct(%8199), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head
  %8260 : Float(1, 5, 500, strides=[2500, 500, 1], requires_grad=0, device=cuda:0) = aten::cat(%8258, %8530), scope: containers.TrtBevFusionCameraOnlyContainer::/projects.BEVFusion.bevfusion.bevfusion_head.BEVFusionHead::bbox_head # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:24:0
  %8270 : Float(1, 5, 500, strides=[2500, 500, 1], requires_grad=0, device=cuda:0) = aten::sigmoid(%8260), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:90:0
  %8272 : Long(device=cpu) = aten::size(%8270, %8516), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:91:0
  %8275 : Long(1, 500, 5, strides=[2500, 5, 1], requires_grad=0, device=cuda:0) = aten::one_hot(%7363, %8272), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:91:0
  %8280 : Long(1, 5, 500, strides=[2500, 1, 5], requires_grad=0, device=cuda:0) = aten::permute(%8275, %8364), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:91:0
  %8281 : Float(1, 5, 500, strides=[2500, 500, 1], requires_grad=0, device=cuda:0) = aten::mul(%8270, %8242), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:92:0
  %8282 : Float(1, 5, 500, strides=[2500, 500, 1], requires_grad=0, device=cuda:0) = aten::mul(%8281, %8280), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:92:0
  %8285 : Float(5, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::select(%8282, %8515, %8515), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:93:0
  %8288 : Float(500, strides=[1], requires_grad=0, device=cuda:0), %8289 : Long(500, strides=[1], requires_grad=0, device=cuda:0) = aten::max(%8285, %8515, %8520), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:93:0
  %8292 : Float(2, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::select(%8245, %8515, %8515), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:96:0
  %8295 : Float(1, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::select(%8248, %8515, %8515), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:96:0
  %8298 : Float(3, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::select(%8251, %8515, %8515), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:96:0
  %8301 : Float(2, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::select(%8254, %8515, %8515), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:96:0
  %8304 : Float(2, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::select(%8257, %8515, %8515), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:96:0
  %8305 : Tensor[] = prim::ListConstruct(%8292, %8295, %8298, %8301, %8304), scope: containers.TrtBevFusionCameraOnlyContainer::
  %8307 : Float(10, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::cat(%8305, %8515), scope: containers.TrtBevFusionCameraOnlyContainer:: # /opt/conda/lib/python3.11/site-packages/mmdeploy/pytorch/functions/cat.py:24:0
  %8310 : Long(500, strides=[1], requires_grad=0, device=cuda:0) = aten::select(%7363, %8515, %8515), scope: containers.TrtBevFusionCameraOnlyContainer:: # /workspace/projects/BEVFusion/deploy/containers.py:100:0
  return (%8307, %8288, %8310)

